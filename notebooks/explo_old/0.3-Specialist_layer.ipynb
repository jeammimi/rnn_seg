{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.objectives import categorical_crossentropy\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "#categorical_crossentropy??\n",
    "#Loss:\n",
    "\n",
    "\n",
    "perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "perm = np.array(perm,dtype=np.int)\n",
    "perm += 3\n",
    "#print perm\n",
    "\n",
    "test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "eps =1e-7\n",
    "test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "             [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "def perm_loss(y_true,y_pred):\n",
    "    def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "        #return  perm[T.cast(m,\"int32\")]\n",
    "        return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "    #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "    perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                     [0, 1, 2, 4, 5, 3, 6],\n",
    "                     [0, 1, 2, 5, 4, 3, 6],\n",
    "                     [0, 1, 2, 3, 5, 4, 6],\n",
    "                     [0, 1, 2, 4, 3, 5, 6],\n",
    "                     [0, 1, 2, 5, 3, 4, 6]],dtype=np.int)\n",
    "    \n",
    "    \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                     [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "    seq = T.arange(len(perm))\n",
    "    result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "                             sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "    return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "#r =perm_loss(test_true,test_pred).eval()\n",
    "#print r\n",
    "#print r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras import backend as K\n",
    "from keras import activations, initializations\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    #print \"v1\"\n",
    "    from keras import activations, initializations, regularizers\n",
    "    from keras.engine import Layer, InputSpec\n",
    "    from keras.layers.recurrent import time_distributed_dense\n",
    "\n",
    "\n",
    "    class BiLSTMv1(Recurrent):\n",
    "        '''Long-Short Term Memory unit - Hochreiter 1997.\n",
    "\n",
    "        For a step-by-step description of the algorithm, see\n",
    "        [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "        # Arguments\n",
    "            output_dim: dimension of the internal projections and the final output.\n",
    "            init: weight initialization function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [initializations](../initializations.md)).\n",
    "            inner_init: initialization function of the inner cells.\n",
    "            forget_bias_init: initialization function for the bias of the forget gate.\n",
    "                [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "                recommend initializing with ones.\n",
    "            activation: activation function.\n",
    "                Can be the name of an existing function (str),\n",
    "                or a Theano function (see: [activations](../activations.md)).\n",
    "            inner_activation: activation function for the inner cells.\n",
    "            W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "            U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "                (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "            b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "                applied to the bias.\n",
    "            dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "            dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "\n",
    "        # References\n",
    "            - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "            - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "            - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "            - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "        '''\n",
    "        def __init__(self, output_dim,\n",
    "                     init='glorot_uniform', inner_init='orthogonal',\n",
    "                     forget_bias_init='one', activation='tanh',\n",
    "                     inner_activation='hard_sigmoid',\n",
    "                     W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                     dropout_W=0., dropout_U=0.,close=False, **kwargs):\n",
    "            self.output_dim = output_dim\n",
    "            self.init = initializations.get(init)\n",
    "            self.inner_init = initializations.get(inner_init)\n",
    "            self.forget_bias_init = initializations.get(forget_bias_init)\n",
    "            self.activation = activations.get(activation)\n",
    "            self.inner_activation = activations.get(inner_activation)\n",
    "            self.W_regularizer = regularizers.get(W_regularizer)\n",
    "            self.U_regularizer = regularizers.get(U_regularizer)\n",
    "            self.b_regularizer = regularizers.get(b_regularizer)\n",
    "            self.dropout_W, self.dropout_U = dropout_W, dropout_U\n",
    "            self.close=close\n",
    "\n",
    "            if self.dropout_W or self.dropout_U:\n",
    "                self.uses_learning_phase = True\n",
    "            super(BiLSTMv1, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.input_spec = [InputSpec(shape=input_shape)]\n",
    "            input_dim = input_shape[2]\n",
    "            self.input_dim = input_dim\n",
    "\n",
    "            if self.stateful:\n",
    "                self.reset_states()\n",
    "            else:\n",
    "                # initial states: 2 all-zero tensors of shape (output_dim)\n",
    "                self.states = [None, None]\n",
    "\n",
    "            self.W_i = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_i'.format(self.name))\n",
    "            self.U_i = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_i'.format(self.name))\n",
    "            self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n",
    "\n",
    "            self.W_f = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_f'.format(self.name))\n",
    "            self.U_f = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_f'.format(self.name))\n",
    "            self.b_f = self.forget_bias_init((self.output_dim,),\n",
    "                                             name='{}_b_f'.format(self.name))\n",
    "\n",
    "            self.W_c = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_c'.format(self.name))\n",
    "            self.U_c = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_c'.format(self.name))\n",
    "            self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n",
    "\n",
    "            self.W_o = self.init((input_dim, self.output_dim),\n",
    "                                 name='{}_W_o'.format(self.name))\n",
    "            self.U_o = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                       name='{}_U_o'.format(self.name))\n",
    "            self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n",
    "\n",
    "            if self.close:\n",
    "                self.W_h =  self.init((self.output_dim, self.output_dim),\n",
    "                                      name='{}_W_h'.format(self.name))\n",
    "                self.b_h = K.zeros((self.output_dim,),\n",
    "                                   name='{}_b_h'.format(self.name))\n",
    "\n",
    "            self.regularizers = []\n",
    "            if self.W_regularizer:\n",
    "                if not self.close:\n",
    "                    self.W_regularizer.set_param(K.concatenate([self.W_i,\n",
    "                                                                self.W_f,\n",
    "                                                            self.W_c,\n",
    "                                                            self.W_o]))\n",
    "                if self.close:\n",
    "                     self.W_regularizer.set_param(K.concatenate([self.W_i,\n",
    "                                                            self.W_f,\n",
    "                                                            self.W_c,\n",
    "                                                            self.W_o,\n",
    "                                                            self.W_h]))\n",
    "\n",
    "                self.regularizers.append(self.W_regularizer)\n",
    "            if self.U_regularizer:\n",
    "                self.U_regularizer.set_param(K.concatenate([self.U_i,\n",
    "                                                            self.U_f,\n",
    "                                                            self.U_c,\n",
    "                                                            self.U_o]))\n",
    "                self.regularizers.append(self.U_regularizer)\n",
    "            if self.b_regularizer:\n",
    "                if not self.close:\n",
    "                    self.b_regularizer.set_param(K.concatenate([self.b_i,\n",
    "                                                                self.b_f,\n",
    "                                                                self.b_c,\n",
    "                                                                self.b_o]))\n",
    "                else:\n",
    "                    self.b_regularizer.set_param(K.concatenate([self.b_i,\n",
    "                                                                self.b_f,\n",
    "                                                                self.b_c,\n",
    "                                                                self.b_o,\n",
    "                                                               self.b_h]))\n",
    "                self.regularizers.append(self.b_regularizer)\n",
    "\n",
    "            self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n",
    "                                      self.W_c, self.U_c, self.b_c,\n",
    "                                      self.W_f, self.U_f, self.b_f,\n",
    "                                      self.W_o, self.U_o, self.b_o]\n",
    "            if self.close:\n",
    "                self.trainable_weights += [self.W_h,self.b_h]\n",
    "\n",
    "\n",
    "            if self.initial_weights is not None:\n",
    "                self.set_weights(self.initial_weights)\n",
    "                del self.initial_weights\n",
    "\n",
    "        def reset_states(self):\n",
    "            assert self.stateful, 'Layer must be stateful.'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "            if not input_shape[0]:\n",
    "                raise Exception('If a RNN is stateful, a complete ' +\n",
    "                                'input_shape must be provided (including batch size).')\n",
    "            if hasattr(self, 'states'):\n",
    "                K.set_value(self.states[0],\n",
    "                            np.zeros((input_shape[0], self.output_dim)))\n",
    "                K.set_value(self.states[1],\n",
    "                            np.zeros((input_shape[0], self.output_dim)))\n",
    "            else:\n",
    "                self.states = [K.zeros((input_shape[0], self.output_dim)),\n",
    "                               K.zeros((input_shape[0], self.output_dim))]\n",
    "\n",
    "        def preprocess_input(self, x, train=False):\n",
    "            if self.consume_less == 'cpu':\n",
    "                if train and (0 < self.dropout_W < 1):\n",
    "                    dropout = self.dropout_W\n",
    "                else:\n",
    "                    dropout = 0\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[2]\n",
    "                timesteps = input_shape[1]\n",
    "\n",
    "                x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n",
    "                                             input_dim, self.output_dim, timesteps)\n",
    "                return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "\n",
    "        def call(self,x,mask=None):\n",
    "\n",
    "            self.go_backwards = False\n",
    "            R1 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            self.go_backwards = True\n",
    "            R2 = Recurrent.call(self,x,mask=mask)\n",
    "\n",
    "            if self.return_sequences:\n",
    "                if K._BACKEND == 'tensorflow':\n",
    "                    R2 = tf.reverse(R2,[False,True,False])\n",
    "                else:\n",
    "                    R2 = R2[::,::-1,::]\n",
    "            if self.close:\n",
    "                return  K.dot(R1 + R2 ,self.W_h) + self.b_h\n",
    "            else:\n",
    "                return  R1 / 2 + R2 / 2\n",
    "            \n",
    "        def cell(self,x):\n",
    "            pass\n",
    "\n",
    "        def step(self, x, states):\n",
    "            h_tm1 = states[0]\n",
    "            c_tm1 = states[1]\n",
    "            B_U = states[2]\n",
    "            B_W = states[3]\n",
    "\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            else:\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "            h = o * self.activation(c)\n",
    "            return h, [h, c]\n",
    "\n",
    "        def get_constants(self, x):\n",
    "            constants = []\n",
    "            if 0 < self.dropout_U < 1:\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * self.output_dim, 1)\n",
    "                B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n",
    "                constants.append(B_U)\n",
    "            else:\n",
    "                constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "            if self.consume_less == 'cpu' and 0 < self.dropout_W < 1:\n",
    "                input_shape = self.input_spec[0].shape\n",
    "                input_dim = input_shape[-1]\n",
    "                ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "                ones = K.concatenate([ones] * input_dim, 1)\n",
    "                B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n",
    "                constants.append(B_W)\n",
    "            else:\n",
    "                constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "            return constants\n",
    "\n",
    "        def get_config(self):\n",
    "            config = {\"output_dim\": self.output_dim,\n",
    "                      \"init\": self.init.__name__,\n",
    "                      \"inner_init\": self.inner_init.__name__,\n",
    "                      \"forget_bias_init\": self.forget_bias_init.__name__,\n",
    "                      \"activation\": self.activation.__name__,\n",
    "                      \"inner_activation\": self.inner_activation.__name__,\n",
    "                      \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                      \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n",
    "                      \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                      \"dropout_W\": self.dropout_W,\n",
    "                      \"dropout_U\": self.dropout_U}\n",
    "            base_config = super(BiLSTMv1, self).get_config()\n",
    "            return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "def reverse(X):\n",
    "    return X[::,::,::-1]\n",
    "\n",
    "def output_shape(input_shape):\n",
    "    # here input_shape includes the samples dimension\n",
    "    return input_shape  # shap\n",
    "\n",
    "\n",
    "def sub_mean(X):\n",
    "    xdms = X.shape\n",
    "    return X.reshape(xdms[0])\n",
    "\n",
    "def old_version(ndim=2):\n",
    "\n",
    "    #middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "    graph.add_node(Convolution1D(nb_filter=5,filter_length=4,input_shape=(None,5),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=20, activation='sigmoid',input_shape=(200,10),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"allmost\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "    graph.add_node(Lambda(reverse, output_shape),inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\",\n",
    "                   name=\"reversed0\")\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=20, activation='sigmoid',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"allmost1\",input=\"reversed0\")\n",
    "\n",
    "\n",
    "    graph.add_node(Lambda(reverse, output_shape),input=\"allmost1\",name=\"reversed\")\n",
    "\n",
    "\n",
    "\n",
    "    #Here get the subcategory\n",
    "    graph.add_node(TimeDistributedDense(7,activation=\"softmax\"),inputs=[\"allmost\",\"reversed\"],\n",
    "                   name=\"output0\",merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    ##########################################\n",
    "    #First ehd here\n",
    "    #graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.compile('adadelta', {'output':'categorical_crossentropy' })\n",
    "    ################################################\n",
    "\n",
    "    #Here get the number of category\n",
    "    graph.add_node(LSTM(output_dim=27, activation='softmax',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,27)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category00\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':'categorical_crossentropy',\n",
    "                              'category':'categorical_crossentropy' })\n",
    "    \n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "def reverse(X):\n",
    "    return X[::,::,::-1]\n",
    "\n",
    "def output_shape(input_shape):\n",
    "    # here input_shape includes the samples dimension\n",
    "    return input_shape  # shap\n",
    "\n",
    "\n",
    "def sub_mean(X):\n",
    "    xdms = X.shape\n",
    "    return X.reshape(xdms[0])\n",
    "\n",
    "def old_but_ok(ndim=2):\n",
    "#middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "\n",
    "    graph.add_node(Convolution1D(nb_filter=10,filter_length=4,input_shape=(None,5),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "    inside=50\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=inside, activation='sigmoid',input_shape=(200,15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"1allmost\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Lambda(reverse, output_shape),inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\",\n",
    "                   name=\"reversed0\")\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=inside, activation='sigmoid',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"allmost1\",input=\"reversed0\")\n",
    "\n",
    "    graph.add_node(Lambda(reverse, output_shape),input=\"allmost1\",name=\"reversed\")\n",
    "\n",
    "    #END first\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=inside, activation='sigmoid',input_shape=(200,2*inside+15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"allmost_l2\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"1allmost\",\"reversed\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(Lambda(reverse, output_shape),inputs=[\"input1\",\"input1b\",\"1allmost\",\"reversed\"],merge_mode=\"concat\",\n",
    "                   concat_axis=-1,\n",
    "                   name=\"reversed0_l2\")\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=inside, activation='sigmoid',input_shape=(200,2*inside+15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"allmost1_l2\",input=\"reversed0_l2\")\n",
    "\n",
    "    graph.add_node(Lambda(reverse, output_shape),input=\"allmost1_l2\",name=\"reversed_l2\")\n",
    "\n",
    "    #END second\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"1allmost\",\"reversed\",\"allmost_l2\",\"reversed_l2\"],\n",
    "                   merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(7,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    #First ehd here\n",
    "    #graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.compile('adadelta', {'output':'categorical_crossentropy' })\n",
    "    ################################################\n",
    "\n",
    "    #Here get the number of category\n",
    "    graph.add_node(LSTM(output_dim=12,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0_r\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(LSTM(output_dim=12,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False,go_backwards=True),\n",
    "                       name=\"category0_l\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(12,activation=\"softmax\"),inputs=[\"category0_l\",\"category0_r\"],concat_axis=1,merge_mode=\"concat\",\n",
    "                   name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,12)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "    #graph.load_weights(\"step_check\")\n",
    "    #############################################\n",
    "    #Original end there\n",
    "    #graph.load_weights(\"step_check\")\n",
    "\n",
    "    #graph.add_output(name=\"category\",input=\"category0\")\n",
    "    #graph.compile('adadelta', {'output':'categorical_crossentropy'})\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    #graph.add_node(TimeDistributedDense(1,activation=\"linear\"),input='output0',name=\"output1\")\n",
    "\n",
    "\n",
    "    #graph.load_weights(\"step_check_bigger\")\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category00\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy'})\n",
    "\n",
    "\n",
    "    graph.load_weights(\"old_weights/specialist_4_diff_size_50\")\n",
    "    \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 660 (CNMeM is disabled, CuDNN not available)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from Bilayer import BiLSTM\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "def return_two_layer():\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "\n",
    "    #middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "\n",
    "    graph.add_node(Convolution1D(nb_filter=10,filter_length=4,input_shape=(None,5),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "    inside=50\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\"],\n",
    "                   merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(7,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    #First ehd here\n",
    "    #graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.compile('adadelta', {'output':'categorical_crossentropy' })\n",
    "    ################################################\n",
    "\n",
    "    #Here get the number of category\n",
    "    graph.add_node(BiLSTM(output_dim=12,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(12,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,12)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "    #graph.load_weights(\"step_check\")\n",
    "    #############################################\n",
    "    #Original end there\n",
    "    #graph.load_weights(\"step_check\")\n",
    "\n",
    "    #graph.add_output(name=\"category\",input=\"category0\")\n",
    "    #graph.compile('adadelta', {'output':'categorical_crossentropy'})\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    #graph.add_node(TimeDistributedDense(1,activation=\"linear\"),input='output0',name=\"output1\")\n",
    "\n",
    "\n",
    "    #graph.load_weights(\"step_check_bigger\")\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category00\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy'})\n",
    "\n",
    "    graph.load_weights(\"saved_weights/two_bilayer_without_sub\")\n",
    "    return graph\n",
    "    #graph.load_weights(\"training_general_scale10\")\n",
    "    #############################################\n",
    "    #Second end there\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "#history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "#predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "#graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from Bilayer import BiLSTM\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "def return_three_layer():\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "\n",
    "    #middle = 50\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "\n",
    "    graph.add_node(Convolution1D(nb_filter=10,filter_length=4,input_shape=(None,5),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "    inside=50\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+15),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l3\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l2\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\",\"l3\"],\n",
    "                   merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=27,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,27)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category0\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy'})\n",
    "    \n",
    "    graph.load_weights(\"three_layer_specialist\")\n",
    "    return graph\n",
    "\n",
    "#graph.load_weights(\"training_general_scale10\")\n",
    "#############################################\n",
    "#Second end there\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "#history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "#predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "#graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "#print theano.__version__ , theano.__file__\n",
    "import keras\n",
    "#print keras.__version__, keras.__file__\n",
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from Bilayer import BiLSTM\n",
    "    \n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from keras.backend.common import _EPSILON\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_three_bis(ndim=2,inside=50):\n",
    "\n",
    "    #categorical_crossentropy??\n",
    "    #Loss:\n",
    "\n",
    "\n",
    "    perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "    perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "    perm = np.array(perm,dtype=np.int)\n",
    "    perm += 3\n",
    "\n",
    "    test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "    eps =1e-7\n",
    "    test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "                 [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "    def perm_loss(y_true,y_pred):\n",
    "        def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "            #return  perm[T.cast(m,\"int32\")]\n",
    "            y_pred = T.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "            return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "        #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "        perm = np.array([[0, 1, 2, 3, 4, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 5, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 4, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 3, 5, 4, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 3, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 3, 4, 6] + range(7,10)],dtype=np.int)\n",
    "\n",
    "        \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                         [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "        seq = T.arange(len(perm))\n",
    "        result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "        sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "        return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "    \n",
    "    #middle = 50\n",
    "    add = 0\n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,5+add))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "\n",
    "    graph.add_node(Convolution1D(nb_filter=10,filter_length=4,input_shape=(None,5+add),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,15+add),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+15+add),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True,),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+15+add),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l3\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l2\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\",\"l3\"],merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(4,activation=\"softmax\"),input=\"output0\",\n",
    "                   name=\"output0b\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=27,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,27)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    graph.add_output(name=\"outputtype\",input=\"output0b\")\n",
    "\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category0\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy',\n",
    "                              'outputtype':'categorical_crossentropy'})\n",
    "\n",
    "    #graph.load_weights(\"training_general_scale10\")\n",
    "    #############################################\n",
    "    #Second end there\n",
    "    \n",
    "    if ndim == 2 and inside == 50:\n",
    "        graph.load_weights(\"saved_weights/three_bilayer_sub_bis\")\n",
    "        \n",
    "    if ndim == 3 and inside == 50:\n",
    "        graph.load_weights(\"saved_weights/three_bilayer_sub_bis_3D_isotrope\")\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    return graph\n",
    "\n",
    "    #history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "    #predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "    #graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def return_three_bis_three_level(ndim=2,inside=50):\n",
    "\n",
    "    #categorical_crossentropy??\n",
    "    #Loss:\n",
    "\n",
    "\n",
    "    perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "    perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "    perm = np.array(perm,dtype=np.int)\n",
    "    perm += 3\n",
    "\n",
    "    test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "    eps =1e-7\n",
    "    test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "                 [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "    def perm_loss(y_true,y_pred):\n",
    "        def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "            #return  perm[T.cast(m,\"int32\")]\n",
    "            y_pred = T.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "            return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "        #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "        perm = np.array([[0, 1, 2, 3, 4, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 5, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 4, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 3, 5, 4, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 3, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 3, 4, 6] + range(7,10)],dtype=np.int)\n",
    "\n",
    "        \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                         [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "        seq = T.arange(len(perm))\n",
    "        result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "        sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "        return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "    \n",
    "    #middle = 50\n",
    "    add = 0\n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(200,3*(5+add)))\n",
    "    #graph.add_input(name='input2', input_shape=(None,2))\n",
    "\n",
    "    #nbr_filter = 10\n",
    "\n",
    "    graph.add_node(Convolution1D(nb_filter=10,filter_length=4,input_shape=(None,3*(5+add)),\n",
    "                                 border_mode=\"same\"),input='input1',name=\"conv1\")\n",
    "\n",
    "\n",
    "    graph.add_node(MaxPooling1D(pool_length=2),\n",
    "                   input='conv1',name=\"max1\")\n",
    "\n",
    "    graph.add_node(UpSampling1D(length=2),\n",
    "                   input='max1',name=\"input1b\")\n",
    "\n",
    "\n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,10+3*(5+add)),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",inputs=[\"input1\",\"input1b\"],concat_axis=-1,merge_mode=\"concat\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+10+3*(5+add)),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True,),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',input_shape=(200,inside+10+3*(5+add)),\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l3\",\n",
    "                       inputs=[\"input1\",\"input1b\",\"l2\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\",\"l3\"],merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(4,activation=\"softmax\"),input=\"output0\",\n",
    "                   name=\"output0b\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=27,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,27)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    graph.add_output(name=\"outputtype\",input=\"output0b\")\n",
    "\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category0\")\n",
    "\n",
    "    graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy',\n",
    "                              'outputtype':'categorical_crossentropy'})\n",
    "\n",
    "    #graph.load_weights(\"training_general_scale10\")\n",
    "    #############################################\n",
    "    #Second end there\n",
    "    \n",
    "   \n",
    "    #############################################\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "#print theano.__version__ , theano.__file__\n",
    "import keras\n",
    "#print keras.__version__, keras.__file__\n",
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    from Bilayer import BiLSTMv1 as BiLSTM\n",
    "else:\n",
    "    from Bilayer import BiLSTM, BiSimpleRNN    \n",
    "    \n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from keras.backend.common import _EPSILON\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_three_bis_simpler(ndim=2,permute=True,extend=0):\n",
    "\n",
    "    #categorical_crossentropy??\n",
    "    #Loss:\n",
    "\n",
    "\n",
    "    perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "    perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "    perm = np.array(perm,dtype=np.int)\n",
    "    perm += 3\n",
    "\n",
    "    test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "    eps =1e-7\n",
    "    test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "                 [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "    def perm_loss(y_true,y_pred):\n",
    "        def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "            #return  perm[T.cast(m,\"int32\")]\n",
    "            y_pred = T.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "            return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "        #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "        perm = np.array([[0, 1, 2, 3, 4, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 5, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 4, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 3, 5, 4, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 3, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 3, 4, 6] + range(7,10)],dtype=np.int)\n",
    "\n",
    "        \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                         [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "        seq = T.arange(len(perm))\n",
    "        result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "        sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "        return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "    \n",
    "    #middle = 50\n",
    "    add = 0\n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "   \n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(None,5+add))\n",
    " \n",
    "\n",
    "    #graph.add_node(Convolution1D(nb_filter=4,filter_length=3,input_shape=(None,2),\n",
    "    #                             border_mode=\"same\"),input='input1',name=\"output0\")\n",
    "\n",
    "    #66,4\n",
    "\n",
    "\n",
    "    #First with 20 of activation\n",
    "\n",
    "    inside=50\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),\n",
    "                       name=\"l1\",input=\"input1\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True,),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=inside, activation='tanh',\n",
    "                        inner_activation='hard_sigmoid',return_sequences=True),name=\"l3\",\n",
    "                       inputs=[\"input1\",\"l2\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\",\"l3\"],merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10 + extend,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "    if permute:\n",
    "        graph.add_node(TimeDistributedDense(4,activation=\"softmax\"),input=\"output0\",\n",
    "                       name=\"output0b\")\n",
    "\n",
    "    graph.add_node(BiLSTM(output_dim=27,\n",
    "                        inner_activation='hard_sigmoid',return_sequences=False),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "    graph.add_node(Reshape((1,27)),input=\"category0\",name = \"category00\")\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "    if permute:\n",
    "        graph.add_output(name=\"outputtype\",input=\"output0b\")\n",
    "\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category0\")\n",
    "\n",
    "    if permute:\n",
    "        graph.compile('adadelta', {'output':perm_loss,\n",
    "                              'category':'categorical_crossentropy',\n",
    "                              'outputtype':'categorical_crossentropy'})\n",
    "    else:\n",
    "        graph.compile('adadelta', {'output':\"categorical_crossentropy\",\n",
    "                              'category':'categorical_crossentropy'})\n",
    "\n",
    "    #graph.load_weights(\"training_general_scale10\")\n",
    "    #############################################\n",
    "    #Second end there\n",
    "   \n",
    "\n",
    "    #############################################\n",
    "\n",
    "    return graph\n",
    "\n",
    "    #history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "    #predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "    #graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "#print theano.__version__ , theano.__file__\n",
    "import keras\n",
    "#print keras.__version__, keras.__file__\n",
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    from Bilayer import BiLSTMv1 as BiLSTM\n",
    "else:\n",
    "    from Bilayer import BiLSTM, BiSimpleRNN    \n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from keras.backend.common import _EPSILON\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_three_paper(ndim=2,inside=50,permutation=True,inputsize=5,simple=False):\n",
    "\n",
    "    #categorical_crossentropy??\n",
    "    #Loss:\n",
    "\n",
    "\n",
    "    perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "    perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "    perm = np.array(perm,dtype=np.int)\n",
    "    perm += 3\n",
    "\n",
    "    test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "    eps =1e-7\n",
    "    test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "                 [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "    def perm_loss(y_true,y_pred):\n",
    "        def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "            #return  perm[T.cast(m,\"int32\")]\n",
    "            y_pred = T.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "            return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "        #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "        perm = np.array([[0, 1, 2, 3, 4, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 5, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 4, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 3, 5, 4, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 3, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 3, 4, 6] + range(7,10)],dtype=np.int)\n",
    "\n",
    "        \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                         [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "        seq = T.arange(len(perm))\n",
    "        result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "        sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "        return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "    \n",
    "    #middle = 50\n",
    "    add = 0\n",
    "    \n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "        \n",
    "    if simple:\n",
    "        Bi = BiSimpleRNN\n",
    "        \n",
    "    else:\n",
    "        Bi = BiLSTM\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(None,inputsize))\n",
    " \n",
    "\n",
    "    graph.add_node(Bi(output_dim=inside,activation='tanh',return_sequences=True,close=True,input_shape=(200,inputsize),),\n",
    "                       name=\"l1\",input=\"input1\")\n",
    "\n",
    "    graph.add_node(Bi(output_dim=inside,input_shape=(200,inputsize),\n",
    "                       return_sequences=True,close=True,activation='tanh'),name=\"l2\",\n",
    "                       inputs=[\"input1\",\"l1\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "    graph.add_node(Bi(output_dim=inside,activation='tanh', input_shape=(200,inputsize),\n",
    "                        return_sequences=True,close=True),name=\"l3\",\n",
    "                       inputs=[\"input1\",\"l2\"],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l1\",\"l2\",\"l3\"],merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "\n",
    "    graph.add_node(Bi(output_dim=27,activation='tanh',return_sequences=False,close=True),\n",
    "                       name=\"category0bi\",input=\"output0\")\n",
    "\n",
    "    graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "\n",
    "\n",
    "    graph.add_output(name=\"output\",input=\"output0\")\n",
    "\n",
    "    #graph.add_output(name=\"rOutput\",input=\"output1\")\n",
    "    graph.add_output(name=\"category\",input=\"category0\")\n",
    "\n",
    "    if permutation:\n",
    "        graph.compile('adadelta', {'output':perm_loss,\n",
    "                                  'category':'categorical_crossentropy'})\n",
    "    else:\n",
    "        graph.compile('adadelta', {'output':'categorical_crossentropy',\n",
    "                                  'category':'categorical_crossentropy'})\n",
    "        \n",
    "\n",
    "    #graph.load_weights(\"training_general_scale10\")\n",
    "    #############################################\n",
    "    #Second end there\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    return graph\n",
    "\n",
    "    #history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "    #predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "    #graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "#print theano.__version__ , theano.__file__\n",
    "import keras\n",
    "#print keras.__version__, keras.__file__\n",
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation,TimeDistributedDense,Merge,Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Convolution1D,MaxPooling1D,UpSampling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "\"\"\"\n",
    "if  int(keras.__version__.split(\".\")[0]) >= 1.0 :\n",
    "    from Bilayer import BiLSTMv1 as BiLSTM\n",
    "    from Bilayer import BiSimpleRNNv1 as BiSimpleRNN\n",
    "\n",
    "else:\n",
    "    from Bilayer import BiLSTM, BiSimpleRNN\n",
    "    \"\"\"\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from keras.backend.common import _EPSILON\n",
    "#from keras.objectives import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_layer_paper(ndim=2,inside=50,permutation=True,inputsize=5,simple=False,\n",
    "                       n_layers=4,category=True,output=True):\n",
    "\n",
    "    #categorical_crossentropy??\n",
    "    #Loss:\n",
    "\n",
    "\n",
    "    perm =[[0,1,2],[1,2,0],[2,1,0],[0,2,1],[1,0,2],[2,0,1]]\n",
    "    perm = [[-3,-2,-1]+iperm for iperm in perm]\n",
    "    perm = np.array(perm,dtype=np.int)\n",
    "    perm += 3\n",
    "\n",
    "    test_true = [[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]],[[0,1],[0,1],[0,1]]]\n",
    "\n",
    "    eps =1e-7\n",
    "    test_pred = [[[1-eps,+eps],[1-eps,eps],[1-eps,eps]],[[eps,1-eps],[eps,1-eps],[eps,1-eps]],\n",
    "                 [[eps,1-eps],[eps,1-eps],[eps,1-eps]]]\n",
    "\n",
    "\n",
    "    def perm_loss(y_true,y_pred):\n",
    "        def loss(m,  y_true, y_pred,perm):\n",
    "\n",
    "            #return  perm[T.cast(m,\"int32\")]\n",
    "            y_pred = T.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "            return T.mean( T.sum(y_true[::,::,perm[m]] * T.log(y_pred),axis=-1),axis=-1)\n",
    "\n",
    "        #perm = np.array([[0,1],[1,0]],dtype=np.int)\n",
    "        perm = np.array([[0, 1, 2, 3, 4, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 5, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 4, 3, 6] + range(7,10),\n",
    "                         [0, 1, 2, 3, 5, 4, 6] + range(7,10),\n",
    "                         [0, 1, 2, 4, 3, 5, 6] + range(7,10),\n",
    "                         [0, 1, 2, 5, 3, 4, 6] + range(7,10)],dtype=np.int)\n",
    "\n",
    "        \"\"\"perm = np.array([[0, 1, 2, 3, 4, 5, 6],\n",
    "                         [0, 1, 2, 3, 4, 5, 6]],dtype=np.int)\"\"\"\n",
    "        seq = T.arange(len(perm))\n",
    "        result, _ = theano.scan(fn=loss, outputs_info=None, \n",
    "        sequences=seq, non_sequences=[y_true, y_pred,perm])\n",
    "        return -T.mean(T.max(result,axis=0)) #T.max(result.dimshuffle(1,2,0),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    def reverse(X):\n",
    "        return X[::,::,::-1]\n",
    "\n",
    "    def output_shape(input_shape):\n",
    "        # here input_shape includes the samples dimension\n",
    "        return input_shape  # shap\n",
    "\n",
    "    def identity(X):\n",
    "        return X\n",
    "\n",
    "    def sub_mean(X):\n",
    "        xdms = X.shape\n",
    "        return X.reshape(xdms[0])\n",
    "    \n",
    "    #middle = 50\n",
    "    add = 0\n",
    "    \n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "        \n",
    "    if simple:\n",
    "        Bi = BiSimpleRNN\n",
    "        \n",
    "    else:\n",
    "        Bi = BiLSTMv1\n",
    "    graph = Graph()\n",
    "    graph.add_input(name='input1', input_shape=(None,inputsize))\n",
    " \n",
    "\n",
    "    graph.add_node(Bi(output_dim=inside,activation='tanh',return_sequences=True,close=True,input_shape=(200,inputsize),),\n",
    "                       name=\"l1\",input=\"input1\")\n",
    "    \n",
    "    \n",
    "    for layer in range(2,n_layers+1):\n",
    "\n",
    "        graph.add_node(Bi(output_dim=inside,input_shape=(200,inputsize),\n",
    "                           return_sequences=True,close=True,activation='tanh'),name=\"l%i\"%layer,\n",
    "                           inputs=[\"input1\",\"l%i\"%(layer-1)],merge_mode=\"concat\",concat_axis=-1)\n",
    "\n",
    "\n",
    "    graph.add_node(Dropout(0.4),inputs=[\"l%i\"%layer for layer in range(1,n_layers+1)],\n",
    "                   merge_mode=\"concat\",concat_axis=-1,name=\"output0_drop\")\n",
    "    #Here get the subcategory\n",
    "\n",
    "    graph.add_node(TimeDistributedDense(10,activation=\"softmax\"),input=\"output0_drop\",\n",
    "                   name=\"output0\")\n",
    "\n",
    "\n",
    "    \n",
    "    res = {}\n",
    "\n",
    "    if category:\n",
    "        graph.add_node(Bi(output_dim=27,activation='tanh',return_sequences=False,close=True),\n",
    "                           name=\"category0bi\",input=\"output0\")\n",
    "        graph.add_node(Dense(27,activation=\"softmax\"),input=\"category0bi\",name=\"category0\")\n",
    "        graph.add_output(name=\"category\",input=\"category0\")\n",
    "        res['category'] = 'categorical_crossentropy'\n",
    "    \n",
    "    if output:\n",
    "        graph.add_output(name=\"output\",input=\"output0\")\n",
    "\n",
    "        if permutation:\n",
    "            res['output'] = perm_loss\n",
    "        else:\n",
    "            res['output'] = 'categorical_crossentropy'\n",
    "        \n",
    "\n",
    "    graph.compile('adadelta', res)\n",
    "        \n",
    "\n",
    "    return graph\n",
    "\n",
    "    #history = graph.fit({'input1':X_train[::,1], 'input2':X2_train[::0], 'output':y_train}, nb_epoch=10)\n",
    "    #predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}\n",
    "    #graph.save_weights(\"step_check\",overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbona/miniconda3/envs/Keras1/lib/python2.7/site-packages/keras/layers/core.py:1112: UserWarning: TimeDistributedDense is deprecated, please use TimeDistributed(Dense(...)) instead.\n",
      "  warnings.warn('TimeDistributedDense is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "graph = return_layer_paper(n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input1 (InputLayer)              (None, None, 5)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "l1 (BiLSTMv1)                    (None, None, 50)      13750       input1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_inputs_for_l2 (Merge)      (None, None, 55)      0           input1[0][0]                     \n",
      "                                                                   l1[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "l2 (BiLSTMv1)                    (None, None, 50)      23750       merge_inputs_for_l2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "merge_inputs_for_l3 (Merge)      (None, None, 55)      0           input1[0][0]                     \n",
      "                                                                   l2[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "l3 (BiLSTMv1)                    (None, None, 50)      23750       merge_inputs_for_l3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "merge_inputs_for_output0_drop (Me(None, None, 150)     0           l1[0][0]                         \n",
      "                                                                   l2[0][0]                         \n",
      "                                                                   l3[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "output0_drop (Dropout)           (None, None, 150)     0           merge_inputs_for_output0_drop[0][\n",
      "____________________________________________________________________________________________________\n",
      "output (TimeDistributedDense)    (None, None, 10)      1510        output0_drop[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "category0bi (BiLSTMv1)           (None, 27)            4860        output[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "category (Dense)                 (None, 27)            756         category0bi[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 68376\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "graph.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
