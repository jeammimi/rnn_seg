{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]]]\n",
      "[[[ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]]]\n"
     ]
    }
   ],
   "source": [
    "from Toolv1 import MotionGenerator, GenerateTraj,random_rot,traj_to_dist\n",
    "from Toolv1 import diffusive,subdiffusive,directed,accelerated,slowed\n",
    "import numpy as np\n",
    "ndim = 2\n",
    "\n",
    "def add_miss_tracking(traj,N,f=10):\n",
    "    \n",
    "    step = traj[1:]-traj[:-1]\n",
    "    \n",
    "    std = np.average(np.sum(step**2,axis=1)**0.5)\n",
    "    \n",
    "    for i in range(N):\n",
    "        w = np.random.randint(0,len(traj))\n",
    "        traj[w] = np.random.normal(traj[w],f*std)\n",
    "    \n",
    "    return traj\n",
    "\n",
    "\n",
    "def generate_N_nstep(N,nstep):\n",
    "    add = 0\n",
    "    ndim = 2\n",
    "    if ndim == 3:\n",
    "        add = 1\n",
    "    size = nstep\n",
    "    \n",
    "    X_train = np.zeros((N,size,5))\n",
    "    Y_trains_b = np.zeros((N,size,1))\n",
    "    Y_trains_s = np.zeros((N,size,1))\n",
    "\n",
    "    Y_train_traj = []\n",
    "\n",
    "    #12\n",
    "    for i in range(N):\n",
    "    #for i in range(1000):\n",
    "\n",
    "        #if i % 1000 == 0:\n",
    "        #    print i\n",
    "        sigma = max(np.random.normal(0.5,1),0.05)\n",
    "        step = max(np.random.normal(1,1),0.2)\n",
    "        tryagain = True\n",
    "        while tryagain:\n",
    "            try:\n",
    "\n",
    "\n",
    "                clean=False\n",
    "                \n",
    "                time=size\n",
    "                ndim=2\n",
    "                list_generator = [MotionGenerator(time,ndim,\n",
    "                                                  parameters=np.random.rand(3),\n",
    "                                                  generate_motion=diffusive),\n",
    "                                   MotionGenerator(time,ndim,\n",
    "                                                  parameters=np.random.rand(3),\n",
    "                                                  generate_motion=directed),\n",
    "                                   MotionGenerator(time,ndim,\n",
    "                                                  parameters=np.random.rand(3),\n",
    "                                                  generate_motion=subdiffusive),\n",
    "                \n",
    "                                  MotionGenerator(time,ndim,\n",
    "                                                  parameters=np.random.rand(3),\n",
    "                                                  generate_motion=accelerated),\n",
    "                                  MotionGenerator(time,ndim,\n",
    "                                                  parameters=np.random.rand(3),\n",
    "                                                  generate_motion=slowed),\n",
    "                                   ]\n",
    "\n",
    "                A = GenerateTraj(time,list_max_possible=[3,3,3,3,3],list_generator=list_generator)\n",
    "                \n",
    "                m0 = np.array(A.sequence) == \"0_0\"\n",
    "                m1 = np.array(A.sequence) == \"0_1\"\n",
    "                m2 = np.array(A.sequence) == \"0_2\"\n",
    " \n",
    "                def map_sequence(sequence):\n",
    "                    ns = []\n",
    "                    for iseque in  sequence:\n",
    "                        i0,j0 = map(int,iseque.split(\"_\"))\n",
    "                        ns.append(i0)\n",
    "                    return ns\n",
    "                \n",
    "                real_traj = A.traj\n",
    "                sc = map_sequence(A.sequence)\n",
    "\n",
    "                alpharot = 2*3.14*np.random.random()\n",
    "                \n",
    "\n",
    "                \n",
    "                real_traj  = random_rot(real_traj,alpharot,ndim=ndim)\n",
    "                \n",
    "\n",
    "                #print A.sequence\n",
    "                #print real_traj.shape\n",
    "                alligned_traj,normed,alpha,_ = traj_to_dist(real_traj,ndim=ndim)\n",
    "                \n",
    "                nzeros = np.random.randint(0,10)\n",
    "                Z = []\n",
    "                for _ in range(nzeros):\n",
    "                    Z.append(np.random.randint(len(sc)-1))\n",
    "                    sc[Z[-1]] = 5\n",
    "           \n",
    "                Z= list(set(Z)) \n",
    "                \n",
    "                for zero in Z:\n",
    "                    normed[zero,::] = 0\n",
    "                    \n",
    "                    \n",
    "                #print  alligned_traj.shape ,len(sc)\n",
    "            \n",
    "                tryagain=False\n",
    "                \n",
    "                \n",
    "            except IndexError:\n",
    "                tryagain=True\n",
    "                \n",
    "        Y_train_traj.append(real_traj)\n",
    "        #print X_train.shape\n",
    "        X_train[i] = normed\n",
    "        \n",
    "      \n",
    "        Y_trains_b[i][np.array(sc,dtype=np.int) == 0 ] = 1\n",
    "        #print m0,sc\n",
    "\n",
    "        Y_trains_s[i][m0] = np.mean(normed[m0,1])\n",
    "        Y_trains_s[i][m1] = np.mean(normed[m1,1])\n",
    "        Y_trains_s[i][m2] = np.mean(normed[m2,1])\n",
    "\n",
    "\n",
    "        if Z != []:\n",
    "            Y_trains_s[i][np.array(Z)] = 0\n",
    "        #print sc\n",
    "\n",
    "    \n",
    "    return X_train,Y_trains_b,Y_trains_s,Y_train_traj\n",
    "\n",
    "R = generate_N_nstep(1,10)\n",
    "print R[1]\n",
    "print R[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='adadelta',\\n              loss={ 'brownian': 'binary_crossentropy'})\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this returns a tensor\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge,TimeDistributed\n",
    "from Bilayer import BiLSTMv1 as BiLSTM\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inputs = Input(shape=(None,5),name=\"Input\")\n",
    "\n",
    "l1 = BiLSTM(output_dim=20,activation='tanh',return_sequences=True)(inputs)\n",
    "l2 = BiLSTM(output_dim=20,activation='tanh',return_sequences=True)(merge([inputs,l1],mode=\"concat\"))\n",
    "l3 = BiLSTM(output_dim=20,activation='tanh',return_sequences=True)(merge([inputs,l2],mode=\"concat\"))\n",
    "\n",
    "brownian = BiLSTM(output_dim=20,activation='tanh',return_sequences=True,name=\"brownian_i\")(merge([inputs,l1,l2,l3],mode=\"concat\"))\n",
    "brownian = TimeDistributed(Dense(1,activation=\"sigmoid\"),name=\"brownian\")(brownian)\n",
    "sigma = BiLSTM(output_dim=20,activation='tanh',return_sequences=True,close=True,name=\"sigma_i\")(merge([inputs,l1,l2,l3,brownian],mode=\"concat\"))\n",
    "sigma = TimeDistributed(Dense(1,activation=\"relu\"),name=\"sigma\")(sigma)\n",
    "\n",
    "model = Model(input=[inputs],output=[brownian,sigma])#,sigma])\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss={'sigma': 'mean_squared_error', 'brownian': 'binary_crossentropy'},\n",
    "              loss_weights={'sigma': .1, 'brownian': .9})\n",
    "\"\"\"\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss={ 'brownian': 'binary_crossentropy'})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/home/jarbona/cluster_theano/ftest_7_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]\n",
      "\n",
      " [[ 0.50901252]\n",
      "  [ 0.50268888]\n",
      "  [ 0.49906114]\n",
      "  [ 0.49648401]\n",
      "  [ 0.49425328]\n",
      "  [ 0.4921411 ]\n",
      "  [ 0.49010983]\n",
      "  [ 0.48817432]\n",
      "  [ 0.48634967]\n",
      "  [ 0.48463738]]]\n",
      "\n",
      "[[[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]\n",
      "\n",
      " [[ 1.10878837]\n",
      "  [ 1.32259488]\n",
      "  [ 1.48127103]\n",
      "  [ 1.59263802]\n",
      "  [ 1.66942132]\n",
      "  [ 1.72317576]\n",
      "  [ 1.7619704 ]\n",
      "  [ 1.79094148]\n",
      "  [ 1.81333268]\n",
      "  [ 1.83123517]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import cPickle\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    #losses = []\n",
    "    #val_losses = []\n",
    "    def __init__(self,name):\n",
    "        super(LossHistory, self).__init__()\n",
    "        self.name=name\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \n",
    "        pass\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        #self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        cPickle.dump((self.losses,self.val_losses), open(self.name, 'wb')) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 42s - loss: 0.3782 - brownian_loss: 0.3824 - sigma_loss: 0.3404 - val_loss: 0.3952 - val_brownian_loss: 0.3996 - val_sigma_loss: 0.3555\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3771 - brownian_loss: 0.3798 - sigma_loss: 0.3529 - val_loss: 0.3821 - val_brownian_loss: 0.3856 - val_sigma_loss: 0.3508\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 118s - loss: 0.3849 - brownian_loss: 0.3869 - sigma_loss: 0.3673 - val_loss: 0.3575 - val_brownian_loss: 0.3581 - val_sigma_loss: 0.3521\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3488 - brownian_loss: 0.3537 - sigma_loss: 0.3048 - val_loss: 0.3277 - val_brownian_loss: 0.3339 - val_sigma_loss: 0.2714\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 165s - loss: 0.3791 - brownian_loss: 0.3798 - sigma_loss: 0.3732 - val_loss: 0.3761 - val_brownian_loss: 0.3780 - val_sigma_loss: 0.3595\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3771 - brownian_loss: 0.3811 - sigma_loss: 0.3414 - val_loss: 0.3711 - val_brownian_loss: 0.3722 - val_sigma_loss: 0.3615\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 77s - loss: 0.3793 - brownian_loss: 0.3813 - sigma_loss: 0.3612 - val_loss: 0.3794 - val_brownian_loss: 0.3849 - val_sigma_loss: 0.3297\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 118s - loss: 0.3882 - brownian_loss: 0.3894 - sigma_loss: 0.3769 - val_loss: 0.3623 - val_brownian_loss: 0.3627 - val_sigma_loss: 0.3585\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3583 - brownian_loss: 0.3635 - sigma_loss: 0.3121 - val_loss: 0.3518 - val_brownian_loss: 0.3544 - val_sigma_loss: 0.3280\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3845 - brownian_loss: 0.3859 - sigma_loss: 0.3717 - val_loss: 0.3951 - val_brownian_loss: 0.3944 - val_sigma_loss: 0.4013\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3743 - brownian_loss: 0.3786 - sigma_loss: 0.3355 - val_loss: 0.3453 - val_brownian_loss: 0.3491 - val_sigma_loss: 0.3104\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 78s - loss: 0.3781 - brownian_loss: 0.3813 - sigma_loss: 0.3498 - val_loss: 0.3860 - val_brownian_loss: 0.3913 - val_sigma_loss: 0.3377\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 118s - loss: 0.3848 - brownian_loss: 0.3867 - sigma_loss: 0.3670 - val_loss: 0.3906 - val_brownian_loss: 0.3886 - val_sigma_loss: 0.4090\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 24s - loss: 0.3532 - brownian_loss: 0.3583 - sigma_loss: 0.3078 - val_loss: 0.3416 - val_brownian_loss: 0.3503 - val_sigma_loss: 0.2639\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3842 - brownian_loss: 0.3852 - sigma_loss: 0.3755 - val_loss: 0.3918 - val_brownian_loss: 0.3931 - val_sigma_loss: 0.3805\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3667 - brownian_loss: 0.3706 - sigma_loss: 0.3321 - val_loss: 0.3719 - val_brownian_loss: 0.3761 - val_sigma_loss: 0.3345\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 79s - loss: 0.3724 - brownian_loss: 0.3751 - sigma_loss: 0.3482 - val_loss: 0.3865 - val_brownian_loss: 0.3870 - val_sigma_loss: 0.3821\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 120s - loss: 0.3841 - brownian_loss: 0.3855 - sigma_loss: 0.3711 - val_loss: 0.3342 - val_brownian_loss: 0.3372 - val_sigma_loss: 0.3072\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3481 - brownian_loss: 0.3531 - sigma_loss: 0.3030 - val_loss: 0.3417 - val_brownian_loss: 0.3440 - val_sigma_loss: 0.3207\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3898 - brownian_loss: 0.3908 - sigma_loss: 0.3807 - val_loss: 0.3762 - val_brownian_loss: 0.3780 - val_sigma_loss: 0.3600\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3690 - brownian_loss: 0.3732 - sigma_loss: 0.3309 - val_loss: 0.3625 - val_brownian_loss: 0.3667 - val_sigma_loss: 0.3248\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3682 - brownian_loss: 0.3711 - sigma_loss: 0.3427 - val_loss: 0.3462 - val_brownian_loss: 0.3504 - val_sigma_loss: 0.3083\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3791 - brownian_loss: 0.3809 - sigma_loss: 0.3625 - val_loss: 0.3415 - val_brownian_loss: 0.3418 - val_sigma_loss: 0.3388\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3464 - brownian_loss: 0.3514 - sigma_loss: 0.3014 - val_loss: 0.3428 - val_brownian_loss: 0.3467 - val_sigma_loss: 0.3083\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3821 - brownian_loss: 0.3844 - sigma_loss: 0.3619 - val_loss: 0.3230 - val_brownian_loss: 0.3261 - val_sigma_loss: 0.2952\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3724 - brownian_loss: 0.3759 - sigma_loss: 0.3401 - val_loss: 0.3931 - val_brownian_loss: 0.3949 - val_sigma_loss: 0.3761\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3707 - brownian_loss: 0.3736 - sigma_loss: 0.3451 - val_loss: 0.3643 - val_brownian_loss: 0.3688 - val_sigma_loss: 0.3231\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3795 - brownian_loss: 0.3812 - sigma_loss: 0.3643 - val_loss: 0.3621 - val_brownian_loss: 0.3641 - val_sigma_loss: 0.3441\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3410 - brownian_loss: 0.3455 - sigma_loss: 0.3006 - val_loss: 0.3859 - val_brownian_loss: 0.3873 - val_sigma_loss: 0.3732\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3851 - brownian_loss: 0.3862 - sigma_loss: 0.3752 - val_loss: 0.3497 - val_brownian_loss: 0.3538 - val_sigma_loss: 0.3126\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3665 - brownian_loss: 0.3702 - sigma_loss: 0.3338 - val_loss: 0.3867 - val_brownian_loss: 0.3885 - val_sigma_loss: 0.3713\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3736 - brownian_loss: 0.3762 - sigma_loss: 0.3497 - val_loss: 0.3979 - val_brownian_loss: 0.4029 - val_sigma_loss: 0.3527\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3758 - brownian_loss: 0.3779 - sigma_loss: 0.3568 - val_loss: 0.3507 - val_brownian_loss: 0.3523 - val_sigma_loss: 0.3358\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 24s - loss: 0.3468 - brownian_loss: 0.3515 - sigma_loss: 0.3043 - val_loss: 0.3323 - val_brownian_loss: 0.3397 - val_sigma_loss: 0.2657\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 156s - loss: 0.3809 - brownian_loss: 0.3823 - sigma_loss: 0.3683 - val_loss: 0.3819 - val_brownian_loss: 0.3800 - val_sigma_loss: 0.3989\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3571 - brownian_loss: 0.3603 - sigma_loss: 0.3279 - val_loss: 0.3515 - val_brownian_loss: 0.3577 - val_sigma_loss: 0.2954\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 78s - loss: 0.3741 - brownian_loss: 0.3767 - sigma_loss: 0.3507 - val_loss: 0.3688 - val_brownian_loss: 0.3714 - val_sigma_loss: 0.3452\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 123s - loss: 0.3789 - brownian_loss: 0.3809 - sigma_loss: 0.3611 - val_loss: 0.4089 - val_brownian_loss: 0.4112 - val_sigma_loss: 0.3880\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3429 - brownian_loss: 0.3470 - sigma_loss: 0.3068 - val_loss: 0.3925 - val_brownian_loss: 0.4009 - val_sigma_loss: 0.3175\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3817 - brownian_loss: 0.3834 - sigma_loss: 0.3663 - val_loss: 0.3984 - val_brownian_loss: 0.3999 - val_sigma_loss: 0.3852\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3626 - brownian_loss: 0.3662 - sigma_loss: 0.3303 - val_loss: 0.3836 - val_brownian_loss: 0.3913 - val_sigma_loss: 0.3143\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3763 - brownian_loss: 0.3795 - sigma_loss: 0.3466 - val_loss: 0.3748 - val_brownian_loss: 0.3768 - val_sigma_loss: 0.3573\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3787 - brownian_loss: 0.3805 - sigma_loss: 0.3626 - val_loss: 0.3232 - val_brownian_loss: 0.3236 - val_sigma_loss: 0.3193\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3400 - brownian_loss: 0.3438 - sigma_loss: 0.3055 - val_loss: 0.3304 - val_brownian_loss: 0.3344 - val_sigma_loss: 0.2946\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3756 - brownian_loss: 0.3772 - sigma_loss: 0.3607 - val_loss: 0.3585 - val_brownian_loss: 0.3605 - val_sigma_loss: 0.3405\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3673 - brownian_loss: 0.3712 - sigma_loss: 0.3322 - val_loss: 0.3265 - val_brownian_loss: 0.3315 - val_sigma_loss: 0.2816\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3661 - brownian_loss: 0.3693 - sigma_loss: 0.3374 - val_loss: 0.3636 - val_brownian_loss: 0.3697 - val_sigma_loss: 0.3091\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3688 - brownian_loss: 0.3712 - sigma_loss: 0.3474 - val_loss: 0.3502 - val_brownian_loss: 0.3530 - val_sigma_loss: 0.3251\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3456 - brownian_loss: 0.3494 - sigma_loss: 0.3114 - val_loss: 0.3201 - val_brownian_loss: 0.3258 - val_sigma_loss: 0.2688\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3830 - brownian_loss: 0.3856 - sigma_loss: 0.3603 - val_loss: 0.3859 - val_brownian_loss: 0.3877 - val_sigma_loss: 0.3694\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3679 - brownian_loss: 0.3715 - sigma_loss: 0.3354 - val_loss: 0.3634 - val_brownian_loss: 0.3671 - val_sigma_loss: 0.3296\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3641 - brownian_loss: 0.3667 - sigma_loss: 0.3406 - val_loss: 0.3856 - val_brownian_loss: 0.3871 - val_sigma_loss: 0.3726\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3748 - brownian_loss: 0.3772 - sigma_loss: 0.3529 - val_loss: 0.3916 - val_brownian_loss: 0.3927 - val_sigma_loss: 0.3817\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3471 - brownian_loss: 0.3513 - sigma_loss: 0.3093 - val_loss: 0.3623 - val_brownian_loss: 0.3695 - val_sigma_loss: 0.2970\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 147s - loss: 0.3712 - brownian_loss: 0.3733 - sigma_loss: 0.3518 - val_loss: 0.4110 - val_brownian_loss: 0.4131 - val_sigma_loss: 0.3919\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3640 - brownian_loss: 0.3680 - sigma_loss: 0.3282 - val_loss: 0.3721 - val_brownian_loss: 0.3763 - val_sigma_loss: 0.3343\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3598 - brownian_loss: 0.3633 - sigma_loss: 0.3286 - val_loss: 0.3675 - val_brownian_loss: 0.3750 - val_sigma_loss: 0.2992\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 120s - loss: 0.3654 - brownian_loss: 0.3680 - sigma_loss: 0.3428 - val_loss: 0.3977 - val_brownian_loss: 0.3976 - val_sigma_loss: 0.3982\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3464 - brownian_loss: 0.3519 - sigma_loss: 0.2967 - val_loss: 0.3609 - val_brownian_loss: 0.3620 - val_sigma_loss: 0.3506\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 152s - loss: 0.3700 - brownian_loss: 0.3732 - sigma_loss: 0.3413 - val_loss: 0.3333 - val_brownian_loss: 0.3346 - val_sigma_loss: 0.3213\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3525 - brownian_loss: 0.3554 - sigma_loss: 0.3266 - val_loss: 0.3698 - val_brownian_loss: 0.3717 - val_sigma_loss: 0.3526\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3589 - brownian_loss: 0.3628 - sigma_loss: 0.3244 - val_loss: 0.3653 - val_brownian_loss: 0.3708 - val_sigma_loss: 0.3156\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3653 - brownian_loss: 0.3679 - sigma_loss: 0.3418 - val_loss: 0.3472 - val_brownian_loss: 0.3474 - val_sigma_loss: 0.3458\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3436 - brownian_loss: 0.3470 - sigma_loss: 0.3131 - val_loss: 0.3255 - val_brownian_loss: 0.3312 - val_sigma_loss: 0.2736\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3640 - brownian_loss: 0.3664 - sigma_loss: 0.3423 - val_loss: 0.3809 - val_brownian_loss: 0.3802 - val_sigma_loss: 0.3866\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3581 - brownian_loss: 0.3616 - sigma_loss: 0.3257 - val_loss: 0.3756 - val_brownian_loss: 0.3844 - val_sigma_loss: 0.2965\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3609 - brownian_loss: 0.3645 - sigma_loss: 0.3283 - val_loss: 0.3450 - val_brownian_loss: 0.3480 - val_sigma_loss: 0.3181\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3683 - brownian_loss: 0.3713 - sigma_loss: 0.3405 - val_loss: 0.3675 - val_brownian_loss: 0.3691 - val_sigma_loss: 0.3526\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3380 - brownian_loss: 0.3435 - sigma_loss: 0.2884 - val_loss: 0.3434 - val_brownian_loss: 0.3481 - val_sigma_loss: 0.3010\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3663 - brownian_loss: 0.3683 - sigma_loss: 0.3485 - val_loss: 0.3260 - val_brownian_loss: 0.3301 - val_sigma_loss: 0.2893\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3513 - brownian_loss: 0.3548 - sigma_loss: 0.3198 - val_loss: 0.3771 - val_brownian_loss: 0.3789 - val_sigma_loss: 0.3614\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3657 - brownian_loss: 0.3698 - sigma_loss: 0.3292 - val_loss: 0.3635 - val_brownian_loss: 0.3700 - val_sigma_loss: 0.3045\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3562 - brownian_loss: 0.3586 - sigma_loss: 0.3353 - val_loss: 0.3798 - val_brownian_loss: 0.3842 - val_sigma_loss: 0.3406\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3402 - brownian_loss: 0.3451 - sigma_loss: 0.2966 - val_loss: 0.3561 - val_brownian_loss: 0.3550 - val_sigma_loss: 0.3662\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3630 - brownian_loss: 0.3659 - sigma_loss: 0.3367 - val_loss: 0.3674 - val_brownian_loss: 0.3727 - val_sigma_loss: 0.3195\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3528 - brownian_loss: 0.3570 - sigma_loss: 0.3155 - val_loss: 0.3597 - val_brownian_loss: 0.3642 - val_sigma_loss: 0.3195\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3621 - brownian_loss: 0.3653 - sigma_loss: 0.3335 - val_loss: 0.3094 - val_brownian_loss: 0.3143 - val_sigma_loss: 0.2653\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3448 - brownian_loss: 0.3473 - sigma_loss: 0.3217 - val_loss: 0.3349 - val_brownian_loss: 0.3390 - val_sigma_loss: 0.2983\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3456 - brownian_loss: 0.3501 - sigma_loss: 0.3052 - val_loss: 0.3505 - val_brownian_loss: 0.3560 - val_sigma_loss: 0.3009\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 147s - loss: 0.3591 - brownian_loss: 0.3623 - sigma_loss: 0.3306 - val_loss: 0.3584 - val_brownian_loss: 0.3614 - val_sigma_loss: 0.3320\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3504 - brownian_loss: 0.3541 - sigma_loss: 0.3177 - val_loss: 0.3346 - val_brownian_loss: 0.3391 - val_sigma_loss: 0.2939\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 77s - loss: 0.3475 - brownian_loss: 0.3519 - sigma_loss: 0.3074 - val_loss: 0.3324 - val_brownian_loss: 0.3359 - val_sigma_loss: 0.3011\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.3472 - brownian_loss: 0.3500 - sigma_loss: 0.3217 - val_loss: 0.3771 - val_brownian_loss: 0.3777 - val_sigma_loss: 0.3721\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3397 - brownian_loss: 0.3439 - sigma_loss: 0.3025 - val_loss: 0.3483 - val_brownian_loss: 0.3515 - val_sigma_loss: 0.3186\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.3670 - brownian_loss: 0.3704 - sigma_loss: 0.3366 - val_loss: 0.3486 - val_brownian_loss: 0.3520 - val_sigma_loss: 0.3181\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3543 - brownian_loss: 0.3572 - sigma_loss: 0.3279 - val_loss: 0.3657 - val_brownian_loss: 0.3718 - val_sigma_loss: 0.3109\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3595 - brownian_loss: 0.3631 - sigma_loss: 0.3266 - val_loss: 0.3455 - val_brownian_loss: 0.3482 - val_sigma_loss: 0.3217\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3532 - brownian_loss: 0.3562 - sigma_loss: 0.3263 - val_loss: 0.3629 - val_brownian_loss: 0.3653 - val_sigma_loss: 0.3419\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3517 - brownian_loss: 0.3564 - sigma_loss: 0.3090 - val_loss: 0.2972 - val_brownian_loss: 0.3015 - val_sigma_loss: 0.2585\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3610 - brownian_loss: 0.3647 - sigma_loss: 0.3279 - val_loss: 0.3338 - val_brownian_loss: 0.3395 - val_sigma_loss: 0.2825\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3516 - brownian_loss: 0.3560 - sigma_loss: 0.3127 - val_loss: 0.3339 - val_brownian_loss: 0.3407 - val_sigma_loss: 0.2727\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.3546 - brownian_loss: 0.3588 - sigma_loss: 0.3166 - val_loss: 0.3230 - val_brownian_loss: 0.3269 - val_sigma_loss: 0.2879\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3561 - brownian_loss: 0.3590 - sigma_loss: 0.3302 - val_loss: 0.3370 - val_brownian_loss: 0.3389 - val_sigma_loss: 0.3199\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3497 - brownian_loss: 0.3551 - sigma_loss: 0.3016 - val_loss: 0.3493 - val_brownian_loss: 0.3520 - val_sigma_loss: 0.3257\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3547 - brownian_loss: 0.3579 - sigma_loss: 0.3265 - val_loss: 0.3202 - val_brownian_loss: 0.3213 - val_sigma_loss: 0.3095\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3512 - brownian_loss: 0.3552 - sigma_loss: 0.3151 - val_loss: 0.3589 - val_brownian_loss: 0.3626 - val_sigma_loss: 0.3251\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3474 - brownian_loss: 0.3514 - sigma_loss: 0.3107 - val_loss: 0.3559 - val_brownian_loss: 0.3559 - val_sigma_loss: 0.3560\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3485 - brownian_loss: 0.3516 - sigma_loss: 0.3207 - val_loss: 0.3702 - val_brownian_loss: 0.3709 - val_sigma_loss: 0.3641\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3382 - brownian_loss: 0.3422 - sigma_loss: 0.3021 - val_loss: 0.3407 - val_brownian_loss: 0.3414 - val_sigma_loss: 0.3343\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.3623 - brownian_loss: 0.3645 - sigma_loss: 0.3416 - val_loss: 0.3835 - val_brownian_loss: 0.3878 - val_sigma_loss: 0.3453\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3431 - brownian_loss: 0.3462 - sigma_loss: 0.3153 - val_loss: 0.3157 - val_brownian_loss: 0.3186 - val_sigma_loss: 0.2895\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3457 - brownian_loss: 0.3502 - sigma_loss: 0.3053 - val_loss: 0.3447 - val_brownian_loss: 0.3505 - val_sigma_loss: 0.2923\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3520 - brownian_loss: 0.3555 - sigma_loss: 0.3205 - val_loss: 0.3629 - val_brownian_loss: 0.3637 - val_sigma_loss: 0.3557\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3334 - brownian_loss: 0.3368 - sigma_loss: 0.3032 - val_loss: 0.3422 - val_brownian_loss: 0.3501 - val_sigma_loss: 0.2718\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3561 - brownian_loss: 0.3595 - sigma_loss: 0.3252 - val_loss: 0.3706 - val_brownian_loss: 0.3744 - val_sigma_loss: 0.3365\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3531 - brownian_loss: 0.3577 - sigma_loss: 0.3115 - val_loss: 0.3600 - val_brownian_loss: 0.3674 - val_sigma_loss: 0.2929\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3534 - brownian_loss: 0.3574 - sigma_loss: 0.3171 - val_loss: 0.3356 - val_brownian_loss: 0.3389 - val_sigma_loss: 0.3059\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3585 - brownian_loss: 0.3610 - sigma_loss: 0.3361 - val_loss: 0.3876 - val_brownian_loss: 0.3931 - val_sigma_loss: 0.3379\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3321 - brownian_loss: 0.3368 - sigma_loss: 0.2898 - val_loss: 0.3505 - val_brownian_loss: 0.3573 - val_sigma_loss: 0.2891\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3591 - brownian_loss: 0.3627 - sigma_loss: 0.3261 - val_loss: 0.3461 - val_brownian_loss: 0.3477 - val_sigma_loss: 0.3324\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3515 - brownian_loss: 0.3547 - sigma_loss: 0.3230 - val_loss: 0.3746 - val_brownian_loss: 0.3761 - val_sigma_loss: 0.3612\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3434 - brownian_loss: 0.3478 - sigma_loss: 0.3045 - val_loss: 0.3593 - val_brownian_loss: 0.3641 - val_sigma_loss: 0.3163\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3399 - brownian_loss: 0.3424 - sigma_loss: 0.3177 - val_loss: 0.3361 - val_brownian_loss: 0.3413 - val_sigma_loss: 0.2887\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3420 - brownian_loss: 0.3461 - sigma_loss: 0.3049 - val_loss: 0.3493 - val_brownian_loss: 0.3513 - val_sigma_loss: 0.3312\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3451 - brownian_loss: 0.3484 - sigma_loss: 0.3163 - val_loss: 0.3534 - val_brownian_loss: 0.3582 - val_sigma_loss: 0.3103\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3538 - brownian_loss: 0.3566 - sigma_loss: 0.3288 - val_loss: 0.3192 - val_brownian_loss: 0.3252 - val_sigma_loss: 0.2653\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3479 - brownian_loss: 0.3523 - sigma_loss: 0.3082 - val_loss: 0.3299 - val_brownian_loss: 0.3345 - val_sigma_loss: 0.2881\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3504 - brownian_loss: 0.3536 - sigma_loss: 0.3213 - val_loss: 0.3665 - val_brownian_loss: 0.3694 - val_sigma_loss: 0.3403\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3368 - brownian_loss: 0.3415 - sigma_loss: 0.2952 - val_loss: 0.2896 - val_brownian_loss: 0.2923 - val_sigma_loss: 0.2650\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3450 - brownian_loss: 0.3487 - sigma_loss: 0.3110 - val_loss: 0.3699 - val_brownian_loss: 0.3697 - val_sigma_loss: 0.3713\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3458 - brownian_loss: 0.3499 - sigma_loss: 0.3090 - val_loss: 0.3584 - val_brownian_loss: 0.3637 - val_sigma_loss: 0.3109\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3422 - brownian_loss: 0.3468 - sigma_loss: 0.3006 - val_loss: 0.3354 - val_brownian_loss: 0.3395 - val_sigma_loss: 0.2991\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3474 - brownian_loss: 0.3509 - sigma_loss: 0.3157 - val_loss: 0.3746 - val_brownian_loss: 0.3791 - val_sigma_loss: 0.3342\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3292 - brownian_loss: 0.3339 - sigma_loss: 0.2870 - val_loss: 0.3459 - val_brownian_loss: 0.3482 - val_sigma_loss: 0.3255\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.3485 - brownian_loss: 0.3518 - sigma_loss: 0.3190 - val_loss: 0.3777 - val_brownian_loss: 0.3805 - val_sigma_loss: 0.3520\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3461 - brownian_loss: 0.3496 - sigma_loss: 0.3144 - val_loss: 0.3284 - val_brownian_loss: 0.3335 - val_sigma_loss: 0.2833\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3434 - brownian_loss: 0.3473 - sigma_loss: 0.3077 - val_loss: 0.3611 - val_brownian_loss: 0.3649 - val_sigma_loss: 0.3272\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3395 - brownian_loss: 0.3425 - sigma_loss: 0.3129 - val_loss: 0.3766 - val_brownian_loss: 0.3825 - val_sigma_loss: 0.3233\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3386 - brownian_loss: 0.3426 - sigma_loss: 0.3021 - val_loss: 0.3372 - val_brownian_loss: 0.3420 - val_sigma_loss: 0.2937\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3425 - brownian_loss: 0.3464 - sigma_loss: 0.3075 - val_loss: 0.3504 - val_brownian_loss: 0.3520 - val_sigma_loss: 0.3357\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3462 - brownian_loss: 0.3509 - sigma_loss: 0.3044 - val_loss: 0.3589 - val_brownian_loss: 0.3665 - val_sigma_loss: 0.2902\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3497 - brownian_loss: 0.3540 - sigma_loss: 0.3108 - val_loss: 0.3210 - val_brownian_loss: 0.3250 - val_sigma_loss: 0.2848\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3372 - brownian_loss: 0.3402 - sigma_loss: 0.3097 - val_loss: 0.3570 - val_brownian_loss: 0.3616 - val_sigma_loss: 0.3153\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3290 - brownian_loss: 0.3325 - sigma_loss: 0.2977 - val_loss: 0.3300 - val_brownian_loss: 0.3331 - val_sigma_loss: 0.3018\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3477 - brownian_loss: 0.3512 - sigma_loss: 0.3158 - val_loss: 0.3615 - val_brownian_loss: 0.3649 - val_sigma_loss: 0.3312\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3504 - brownian_loss: 0.3541 - sigma_loss: 0.3170 - val_loss: 0.3339 - val_brownian_loss: 0.3366 - val_sigma_loss: 0.3103\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3357 - brownian_loss: 0.3398 - sigma_loss: 0.2980 - val_loss: 0.3706 - val_brownian_loss: 0.3746 - val_sigma_loss: 0.3348\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3491 - brownian_loss: 0.3523 - sigma_loss: 0.3202 - val_loss: 0.3583 - val_brownian_loss: 0.3636 - val_sigma_loss: 0.3113\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3341 - brownian_loss: 0.3379 - sigma_loss: 0.3002 - val_loss: 0.3204 - val_brownian_loss: 0.3257 - val_sigma_loss: 0.2726\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.3379 - brownian_loss: 0.3415 - sigma_loss: 0.3051 - val_loss: 0.3426 - val_brownian_loss: 0.3418 - val_sigma_loss: 0.3495\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3484 - brownian_loss: 0.3519 - sigma_loss: 0.3173 - val_loss: 0.3563 - val_brownian_loss: 0.3588 - val_sigma_loss: 0.3334\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3430 - brownian_loss: 0.3478 - sigma_loss: 0.2998 - val_loss: 0.3393 - val_brownian_loss: 0.3433 - val_sigma_loss: 0.3029\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3337 - brownian_loss: 0.3372 - sigma_loss: 0.3019 - val_loss: 0.3482 - val_brownian_loss: 0.3521 - val_sigma_loss: 0.3128\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3377 - brownian_loss: 0.3421 - sigma_loss: 0.2984 - val_loss: 0.3357 - val_brownian_loss: 0.3407 - val_sigma_loss: 0.2911\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3409 - brownian_loss: 0.3452 - sigma_loss: 0.3028 - val_loss: 0.3538 - val_brownian_loss: 0.3572 - val_sigma_loss: 0.3227\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3419 - brownian_loss: 0.3457 - sigma_loss: 0.3077 - val_loss: 0.3411 - val_brownian_loss: 0.3443 - val_sigma_loss: 0.3116\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3452 - brownian_loss: 0.3490 - sigma_loss: 0.3108 - val_loss: 0.3655 - val_brownian_loss: 0.3651 - val_sigma_loss: 0.3684\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3329 - brownian_loss: 0.3366 - sigma_loss: 0.2990 - val_loss: 0.3568 - val_brownian_loss: 0.3593 - val_sigma_loss: 0.3345\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3333 - brownian_loss: 0.3369 - sigma_loss: 0.3008 - val_loss: 0.3408 - val_brownian_loss: 0.3453 - val_sigma_loss: 0.2998\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3468 - brownian_loss: 0.3515 - sigma_loss: 0.3041 - val_loss: 0.3541 - val_brownian_loss: 0.3565 - val_sigma_loss: 0.3326\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3328 - brownian_loss: 0.3372 - sigma_loss: 0.2937 - val_loss: 0.3115 - val_brownian_loss: 0.3184 - val_sigma_loss: 0.2493\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3336 - brownian_loss: 0.3377 - sigma_loss: 0.2968 - val_loss: 0.3299 - val_brownian_loss: 0.3316 - val_sigma_loss: 0.3150\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3313 - brownian_loss: 0.3350 - sigma_loss: 0.2979 - val_loss: 0.2973 - val_brownian_loss: 0.3014 - val_sigma_loss: 0.2611\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3397 - brownian_loss: 0.3429 - sigma_loss: 0.3109 - val_loss: 0.3275 - val_brownian_loss: 0.3309 - val_sigma_loss: 0.2967\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3411 - brownian_loss: 0.3449 - sigma_loss: 0.3065 - val_loss: 0.3521 - val_brownian_loss: 0.3543 - val_sigma_loss: 0.3322\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3362 - brownian_loss: 0.3402 - sigma_loss: 0.2999 - val_loss: 0.3489 - val_brownian_loss: 0.3509 - val_sigma_loss: 0.3305\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3300 - brownian_loss: 0.3334 - sigma_loss: 0.2988 - val_loss: 0.3643 - val_brownian_loss: 0.3690 - val_sigma_loss: 0.3219\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3423 - brownian_loss: 0.3469 - sigma_loss: 0.3009 - val_loss: 0.3114 - val_brownian_loss: 0.3173 - val_sigma_loss: 0.2583\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 147s - loss: 0.3412 - brownian_loss: 0.3443 - sigma_loss: 0.3133 - val_loss: 0.3415 - val_brownian_loss: 0.3461 - val_sigma_loss: 0.2993\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3371 - brownian_loss: 0.3411 - sigma_loss: 0.3010 - val_loss: 0.3787 - val_brownian_loss: 0.3843 - val_sigma_loss: 0.3285\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 70s - loss: 0.3300 - brownian_loss: 0.3341 - sigma_loss: 0.2933 - val_loss: 0.3516 - val_brownian_loss: 0.3565 - val_sigma_loss: 0.3073\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3333 - brownian_loss: 0.3367 - sigma_loss: 0.3024 - val_loss: 0.3033 - val_brownian_loss: 0.3079 - val_sigma_loss: 0.2618\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3334 - brownian_loss: 0.3376 - sigma_loss: 0.2959 - val_loss: 0.3426 - val_brownian_loss: 0.3461 - val_sigma_loss: 0.3116\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.3264 - brownian_loss: 0.3296 - sigma_loss: 0.2974 - val_loss: 0.3122 - val_brownian_loss: 0.3171 - val_sigma_loss: 0.2686\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3471 - brownian_loss: 0.3516 - sigma_loss: 0.3062 - val_loss: 0.3601 - val_brownian_loss: 0.3652 - val_sigma_loss: 0.3137\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 70s - loss: 0.3317 - brownian_loss: 0.3360 - sigma_loss: 0.2932 - val_loss: 0.3275 - val_brownian_loss: 0.3342 - val_sigma_loss: 0.2672\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3381 - brownian_loss: 0.3421 - sigma_loss: 0.3017 - val_loss: 0.3403 - val_brownian_loss: 0.3452 - val_sigma_loss: 0.2958\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3362 - brownian_loss: 0.3394 - sigma_loss: 0.3068 - val_loss: 0.3395 - val_brownian_loss: 0.3475 - val_sigma_loss: 0.2678\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.3289 - brownian_loss: 0.3323 - sigma_loss: 0.2982 - val_loss: 0.3379 - val_brownian_loss: 0.3414 - val_sigma_loss: 0.3067\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3492 - brownian_loss: 0.3537 - sigma_loss: 0.3087 - val_loss: 0.3273 - val_brownian_loss: 0.3274 - val_sigma_loss: 0.3260\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3333 - brownian_loss: 0.3376 - sigma_loss: 0.2944 - val_loss: 0.3544 - val_brownian_loss: 0.3586 - val_sigma_loss: 0.3167\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3354 - brownian_loss: 0.3384 - sigma_loss: 0.3083 - val_loss: 0.3199 - val_brownian_loss: 0.3224 - val_sigma_loss: 0.2970\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3308 - brownian_loss: 0.3361 - sigma_loss: 0.2832 - val_loss: 0.3250 - val_brownian_loss: 0.3318 - val_sigma_loss: 0.2639\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3306 - brownian_loss: 0.3338 - sigma_loss: 0.3019 - val_loss: 0.3439 - val_brownian_loss: 0.3498 - val_sigma_loss: 0.2908\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3413 - brownian_loss: 0.3449 - sigma_loss: 0.3084 - val_loss: 0.3393 - val_brownian_loss: 0.3422 - val_sigma_loss: 0.3140\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3272 - brownian_loss: 0.3317 - sigma_loss: 0.2860 - val_loss: 0.3303 - val_brownian_loss: 0.3308 - val_sigma_loss: 0.3259\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3385 - brownian_loss: 0.3421 - sigma_loss: 0.3057 - val_loss: 0.3106 - val_brownian_loss: 0.3140 - val_sigma_loss: 0.2795\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3283 - brownian_loss: 0.3328 - sigma_loss: 0.2882 - val_loss: 0.3225 - val_brownian_loss: 0.3262 - val_sigma_loss: 0.2892\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.3299 - brownian_loss: 0.3331 - sigma_loss: 0.3011 - val_loss: 0.3186 - val_brownian_loss: 0.3193 - val_sigma_loss: 0.3123\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3372 - brownian_loss: 0.3413 - sigma_loss: 0.3006 - val_loss: 0.3542 - val_brownian_loss: 0.3577 - val_sigma_loss: 0.3228\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.3286 - brownian_loss: 0.3325 - sigma_loss: 0.2927 - val_loss: 0.3444 - val_brownian_loss: 0.3503 - val_sigma_loss: 0.2912\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.3259 - brownian_loss: 0.3292 - sigma_loss: 0.2966 - val_loss: 0.3362 - val_brownian_loss: 0.3380 - val_sigma_loss: 0.3205\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3355 - brownian_loss: 0.3391 - sigma_loss: 0.3032 - val_loss: 0.3562 - val_brownian_loss: 0.3606 - val_sigma_loss: 0.3170\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.3302 - brownian_loss: 0.3341 - sigma_loss: 0.2951 - val_loss: 0.2935 - val_brownian_loss: 0.2968 - val_sigma_loss: 0.2636\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3455 - brownian_loss: 0.3498 - sigma_loss: 0.3070 - val_loss: 0.3544 - val_brownian_loss: 0.3600 - val_sigma_loss: 0.3041\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3276 - brownian_loss: 0.3316 - sigma_loss: 0.2914 - val_loss: 0.3598 - val_brownian_loss: 0.3633 - val_sigma_loss: 0.3286\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3211 - brownian_loss: 0.3242 - sigma_loss: 0.2928 - val_loss: 0.3003 - val_brownian_loss: 0.3075 - val_sigma_loss: 0.2362\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3296 - brownian_loss: 0.3346 - sigma_loss: 0.2846 - val_loss: 0.3376 - val_brownian_loss: 0.3421 - val_sigma_loss: 0.2976\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.3299 - brownian_loss: 0.3332 - sigma_loss: 0.2999 - val_loss: 0.3039 - val_brownian_loss: 0.3073 - val_sigma_loss: 0.2729\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3311 - brownian_loss: 0.3349 - sigma_loss: 0.2969 - val_loss: 0.3352 - val_brownian_loss: 0.3390 - val_sigma_loss: 0.3016\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.3349 - brownian_loss: 0.3391 - sigma_loss: 0.2964 - val_loss: 0.3493 - val_brownian_loss: 0.3497 - val_sigma_loss: 0.3458\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3311 - brownian_loss: 0.3345 - sigma_loss: 0.3002 - val_loss: 0.3242 - val_brownian_loss: 0.3275 - val_sigma_loss: 0.2945\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3347 - brownian_loss: 0.3381 - sigma_loss: 0.3040 - val_loss: 0.3845 - val_brownian_loss: 0.3870 - val_sigma_loss: 0.3623\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.3254 - brownian_loss: 0.3294 - sigma_loss: 0.2889 - val_loss: 0.3118 - val_brownian_loss: 0.3168 - val_sigma_loss: 0.2671\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3320 - brownian_loss: 0.3367 - sigma_loss: 0.2894 - val_loss: 0.3145 - val_brownian_loss: 0.3213 - val_sigma_loss: 0.2538\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3293 - brownian_loss: 0.3336 - sigma_loss: 0.2900 - val_loss: 0.3655 - val_brownian_loss: 0.3699 - val_sigma_loss: 0.3261\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3229 - brownian_loss: 0.3259 - sigma_loss: 0.2964 - val_loss: 0.3422 - val_brownian_loss: 0.3453 - val_sigma_loss: 0.3134\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3282 - brownian_loss: 0.3321 - sigma_loss: 0.2933 - val_loss: 0.3505 - val_brownian_loss: 0.3551 - val_sigma_loss: 0.3091\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.3265 - brownian_loss: 0.3296 - sigma_loss: 0.2987 - val_loss: 0.3546 - val_brownian_loss: 0.3607 - val_sigma_loss: 0.3004\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3289 - brownian_loss: 0.3323 - sigma_loss: 0.2983 - val_loss: 0.3314 - val_brownian_loss: 0.3357 - val_sigma_loss: 0.2935\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3215 - brownian_loss: 0.3273 - sigma_loss: 0.2695 - val_loss: 0.3192 - val_brownian_loss: 0.3206 - val_sigma_loss: 0.3072\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.3174 - brownian_loss: 0.3211 - sigma_loss: 0.2843 - val_loss: 0.3241 - val_brownian_loss: 0.3281 - val_sigma_loss: 0.2884\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3318 - brownian_loss: 0.3360 - sigma_loss: 0.2941 - val_loss: 0.3310 - val_brownian_loss: 0.3362 - val_sigma_loss: 0.2843\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3236 - brownian_loss: 0.3272 - sigma_loss: 0.2919 - val_loss: 0.3158 - val_brownian_loss: 0.3202 - val_sigma_loss: 0.2764\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3342 - brownian_loss: 0.3382 - sigma_loss: 0.2977 - val_loss: 0.3275 - val_brownian_loss: 0.3291 - val_sigma_loss: 0.3126\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3240 - brownian_loss: 0.3278 - sigma_loss: 0.2894 - val_loss: 0.2802 - val_brownian_loss: 0.2852 - val_sigma_loss: 0.2348\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3215 - brownian_loss: 0.3253 - sigma_loss: 0.2876 - val_loss: 0.3002 - val_brownian_loss: 0.3058 - val_sigma_loss: 0.2505\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3279 - brownian_loss: 0.3323 - sigma_loss: 0.2883 - val_loss: 0.3454 - val_brownian_loss: 0.3485 - val_sigma_loss: 0.3180\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3195 - brownian_loss: 0.3231 - sigma_loss: 0.2876 - val_loss: 0.3108 - val_brownian_loss: 0.3157 - val_sigma_loss: 0.2660\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3366 - brownian_loss: 0.3409 - sigma_loss: 0.2982 - val_loss: 0.3100 - val_brownian_loss: 0.3165 - val_sigma_loss: 0.2508\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3298 - brownian_loss: 0.3336 - sigma_loss: 0.2956 - val_loss: 0.3272 - val_brownian_loss: 0.3324 - val_sigma_loss: 0.2801\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3175 - brownian_loss: 0.3212 - sigma_loss: 0.2845 - val_loss: 0.2939 - val_brownian_loss: 0.2978 - val_sigma_loss: 0.2584\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3251 - brownian_loss: 0.3289 - sigma_loss: 0.2915 - val_loss: 0.3288 - val_brownian_loss: 0.3352 - val_sigma_loss: 0.2709\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3253 - brownian_loss: 0.3282 - sigma_loss: 0.2992 - val_loss: 0.3147 - val_brownian_loss: 0.3164 - val_sigma_loss: 0.2993\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3323 - brownian_loss: 0.3363 - sigma_loss: 0.2954 - val_loss: 0.3651 - val_brownian_loss: 0.3688 - val_sigma_loss: 0.3313\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3297 - brownian_loss: 0.3341 - sigma_loss: 0.2895 - val_loss: 0.3061 - val_brownian_loss: 0.3098 - val_sigma_loss: 0.2723\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3294 - brownian_loss: 0.3328 - sigma_loss: 0.2985 - val_loss: 0.3316 - val_brownian_loss: 0.3343 - val_sigma_loss: 0.3073\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3368 - brownian_loss: 0.3406 - sigma_loss: 0.3031 - val_loss: 0.3054 - val_brownian_loss: 0.3106 - val_sigma_loss: 0.2583\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3173 - brownian_loss: 0.3205 - sigma_loss: 0.2882 - val_loss: 0.2977 - val_brownian_loss: 0.2999 - val_sigma_loss: 0.2771\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3315 - brownian_loss: 0.3351 - sigma_loss: 0.2984 - val_loss: 0.3121 - val_brownian_loss: 0.3171 - val_sigma_loss: 0.2670\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3219 - brownian_loss: 0.3266 - sigma_loss: 0.2805 - val_loss: 0.2990 - val_brownian_loss: 0.3014 - val_sigma_loss: 0.2781\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3228 - brownian_loss: 0.3266 - sigma_loss: 0.2887 - val_loss: 0.3517 - val_brownian_loss: 0.3557 - val_sigma_loss: 0.3153\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3301 - brownian_loss: 0.3341 - sigma_loss: 0.2936 - val_loss: 0.3184 - val_brownian_loss: 0.3232 - val_sigma_loss: 0.2753\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3153 - brownian_loss: 0.3186 - sigma_loss: 0.2857 - val_loss: 0.2913 - val_brownian_loss: 0.2950 - val_sigma_loss: 0.2580\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3349 - brownian_loss: 0.3395 - sigma_loss: 0.2931 - val_loss: 0.3184 - val_brownian_loss: 0.3217 - val_sigma_loss: 0.2885\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3274 - brownian_loss: 0.3322 - sigma_loss: 0.2842 - val_loss: 0.2927 - val_brownian_loss: 0.2975 - val_sigma_loss: 0.2491\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3197 - brownian_loss: 0.3235 - sigma_loss: 0.2854 - val_loss: 0.3179 - val_brownian_loss: 0.3194 - val_sigma_loss: 0.3043\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3309 - brownian_loss: 0.3355 - sigma_loss: 0.2891 - val_loss: 0.3155 - val_brownian_loss: 0.3190 - val_sigma_loss: 0.2843\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3194 - brownian_loss: 0.3235 - sigma_loss: 0.2832 - val_loss: 0.3353 - val_brownian_loss: 0.3399 - val_sigma_loss: 0.2941\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3371 - brownian_loss: 0.3419 - sigma_loss: 0.2939 - val_loss: 0.3366 - val_brownian_loss: 0.3409 - val_sigma_loss: 0.2976\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3267 - brownian_loss: 0.3307 - sigma_loss: 0.2914 - val_loss: 0.2567 - val_brownian_loss: 0.2616 - val_sigma_loss: 0.2129\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3293 - brownian_loss: 0.3332 - sigma_loss: 0.2940 - val_loss: 0.3219 - val_brownian_loss: 0.3278 - val_sigma_loss: 0.2686\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3394 - brownian_loss: 0.3440 - sigma_loss: 0.2975 - val_loss: 0.3394 - val_brownian_loss: 0.3438 - val_sigma_loss: 0.2996\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3128 - brownian_loss: 0.3164 - sigma_loss: 0.2806 - val_loss: 0.3111 - val_brownian_loss: 0.3162 - val_sigma_loss: 0.2646\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3293 - brownian_loss: 0.3343 - sigma_loss: 0.2843 - val_loss: 0.3763 - val_brownian_loss: 0.3791 - val_sigma_loss: 0.3508\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3303 - brownian_loss: 0.3348 - sigma_loss: 0.2892 - val_loss: 0.3131 - val_brownian_loss: 0.3170 - val_sigma_loss: 0.2780\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3211 - brownian_loss: 0.3250 - sigma_loss: 0.2860 - val_loss: 0.3111 - val_brownian_loss: 0.3141 - val_sigma_loss: 0.2839\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3294 - brownian_loss: 0.3330 - sigma_loss: 0.2967 - val_loss: 0.3493 - val_brownian_loss: 0.3528 - val_sigma_loss: 0.3174\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3131 - brownian_loss: 0.3171 - sigma_loss: 0.2771 - val_loss: 0.3120 - val_brownian_loss: 0.3151 - val_sigma_loss: 0.2845\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3316 - brownian_loss: 0.3364 - sigma_loss: 0.2883 - val_loss: 0.3506 - val_brownian_loss: 0.3566 - val_sigma_loss: 0.2965\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3234 - brownian_loss: 0.3278 - sigma_loss: 0.2835 - val_loss: 0.3345 - val_brownian_loss: 0.3393 - val_sigma_loss: 0.2911\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3237 - brownian_loss: 0.3268 - sigma_loss: 0.2957 - val_loss: 0.3476 - val_brownian_loss: 0.3507 - val_sigma_loss: 0.3193\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3310 - brownian_loss: 0.3352 - sigma_loss: 0.2930 - val_loss: 0.3068 - val_brownian_loss: 0.3103 - val_sigma_loss: 0.2754\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3122 - brownian_loss: 0.3157 - sigma_loss: 0.2809 - val_loss: 0.3220 - val_brownian_loss: 0.3272 - val_sigma_loss: 0.2758\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3295 - brownian_loss: 0.3336 - sigma_loss: 0.2922 - val_loss: 0.3404 - val_brownian_loss: 0.3459 - val_sigma_loss: 0.2913\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3238 - brownian_loss: 0.3283 - sigma_loss: 0.2827 - val_loss: 0.3450 - val_brownian_loss: 0.3497 - val_sigma_loss: 0.3027\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3119 - brownian_loss: 0.3153 - sigma_loss: 0.2820 - val_loss: 0.2890 - val_brownian_loss: 0.2942 - val_sigma_loss: 0.2414\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3248 - brownian_loss: 0.3290 - sigma_loss: 0.2873 - val_loss: 0.3179 - val_brownian_loss: 0.3229 - val_sigma_loss: 0.2729\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3176 - brownian_loss: 0.3212 - sigma_loss: 0.2844 - val_loss: 0.3050 - val_brownian_loss: 0.3063 - val_sigma_loss: 0.2936\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3300 - brownian_loss: 0.3338 - sigma_loss: 0.2956 - val_loss: 0.2875 - val_brownian_loss: 0.2928 - val_sigma_loss: 0.2398\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3169 - brownian_loss: 0.3213 - sigma_loss: 0.2779 - val_loss: 0.3368 - val_brownian_loss: 0.3411 - val_sigma_loss: 0.2982\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3164 - brownian_loss: 0.3202 - sigma_loss: 0.2818 - val_loss: 0.3080 - val_brownian_loss: 0.3126 - val_sigma_loss: 0.2672\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3297 - brownian_loss: 0.3335 - sigma_loss: 0.2957 - val_loss: 0.3290 - val_brownian_loss: 0.3322 - val_sigma_loss: 0.3004\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3154 - brownian_loss: 0.3192 - sigma_loss: 0.2806 - val_loss: 0.2984 - val_brownian_loss: 0.3021 - val_sigma_loss: 0.2657\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3312 - brownian_loss: 0.3353 - sigma_loss: 0.2945 - val_loss: 0.3431 - val_brownian_loss: 0.3491 - val_sigma_loss: 0.2894\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3288 - brownian_loss: 0.3332 - sigma_loss: 0.2885 - val_loss: 0.2962 - val_brownian_loss: 0.2999 - val_sigma_loss: 0.2629\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3144 - brownian_loss: 0.3182 - sigma_loss: 0.2799 - val_loss: 0.3653 - val_brownian_loss: 0.3685 - val_sigma_loss: 0.3360\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3278 - brownian_loss: 0.3315 - sigma_loss: 0.2945 - val_loss: 0.2999 - val_brownian_loss: 0.3055 - val_sigma_loss: 0.2499\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3100 - brownian_loss: 0.3139 - sigma_loss: 0.2750 - val_loss: 0.3534 - val_brownian_loss: 0.3591 - val_sigma_loss: 0.3022\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3268 - brownian_loss: 0.3310 - sigma_loss: 0.2889 - val_loss: 0.3119 - val_brownian_loss: 0.3169 - val_sigma_loss: 0.2670\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3185 - brownian_loss: 0.3233 - sigma_loss: 0.2752 - val_loss: 0.2980 - val_brownian_loss: 0.3005 - val_sigma_loss: 0.2755\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3180 - brownian_loss: 0.3214 - sigma_loss: 0.2878 - val_loss: 0.3320 - val_brownian_loss: 0.3364 - val_sigma_loss: 0.2920\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3275 - brownian_loss: 0.3314 - sigma_loss: 0.2927 - val_loss: 0.2811 - val_brownian_loss: 0.2864 - val_sigma_loss: 0.2335\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3130 - brownian_loss: 0.3166 - sigma_loss: 0.2805 - val_loss: 0.3095 - val_brownian_loss: 0.3143 - val_sigma_loss: 0.2661\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3261 - brownian_loss: 0.3298 - sigma_loss: 0.2926 - val_loss: 0.3145 - val_brownian_loss: 0.3178 - val_sigma_loss: 0.2848\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3157 - brownian_loss: 0.3204 - sigma_loss: 0.2725 - val_loss: 0.2982 - val_brownian_loss: 0.3035 - val_sigma_loss: 0.2505\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3086 - brownian_loss: 0.3124 - sigma_loss: 0.2745 - val_loss: 0.3074 - val_brownian_loss: 0.3094 - val_sigma_loss: 0.2892\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3331 - brownian_loss: 0.3378 - sigma_loss: 0.2909 - val_loss: 0.3607 - val_brownian_loss: 0.3646 - val_sigma_loss: 0.3258\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3092 - brownian_loss: 0.3127 - sigma_loss: 0.2774 - val_loss: 0.3461 - val_brownian_loss: 0.3502 - val_sigma_loss: 0.3085\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3256 - brownian_loss: 0.3297 - sigma_loss: 0.2887 - val_loss: 0.3176 - val_brownian_loss: 0.3225 - val_sigma_loss: 0.2731\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3095 - brownian_loss: 0.3140 - sigma_loss: 0.2692 - val_loss: 0.3186 - val_brownian_loss: 0.3262 - val_sigma_loss: 0.2510\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3145 - brownian_loss: 0.3182 - sigma_loss: 0.2814 - val_loss: 0.2924 - val_brownian_loss: 0.2961 - val_sigma_loss: 0.2590\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3355 - brownian_loss: 0.3386 - sigma_loss: 0.3080 - val_loss: 0.2986 - val_brownian_loss: 0.3008 - val_sigma_loss: 0.2785\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3037 - brownian_loss: 0.3074 - sigma_loss: 0.2703 - val_loss: 0.2788 - val_brownian_loss: 0.2839 - val_sigma_loss: 0.2324\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3228 - brownian_loss: 0.3271 - sigma_loss: 0.2845 - val_loss: 0.3037 - val_brownian_loss: 0.3088 - val_sigma_loss: 0.2581\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3266 - brownian_loss: 0.3309 - sigma_loss: 0.2874 - val_loss: 0.3455 - val_brownian_loss: 0.3501 - val_sigma_loss: 0.3046\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3151 - brownian_loss: 0.3185 - sigma_loss: 0.2847 - val_loss: 0.3086 - val_brownian_loss: 0.3107 - val_sigma_loss: 0.2892\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 24s - loss: 0.3259 - brownian_loss: 0.3305 - sigma_loss: 0.2843 - val_loss: 0.3255 - val_brownian_loss: 0.3328 - val_sigma_loss: 0.2600\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3073 - brownian_loss: 0.3116 - sigma_loss: 0.2687 - val_loss: 0.3073 - val_brownian_loss: 0.3085 - val_sigma_loss: 0.2969\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3256 - brownian_loss: 0.3299 - sigma_loss: 0.2865 - val_loss: 0.3307 - val_brownian_loss: 0.3370 - val_sigma_loss: 0.2741\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3164 - brownian_loss: 0.3214 - sigma_loss: 0.2707 - val_loss: 0.3135 - val_brownian_loss: 0.3180 - val_sigma_loss: 0.2730\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3172 - brownian_loss: 0.3203 - sigma_loss: 0.2893 - val_loss: 0.2804 - val_brownian_loss: 0.2858 - val_sigma_loss: 0.2317\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3274 - brownian_loss: 0.3318 - sigma_loss: 0.2886 - val_loss: 0.3309 - val_brownian_loss: 0.3379 - val_sigma_loss: 0.2680\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.3048 - brownian_loss: 0.3082 - sigma_loss: 0.2737 - val_loss: 0.3020 - val_brownian_loss: 0.3062 - val_sigma_loss: 0.2639\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3241 - brownian_loss: 0.3280 - sigma_loss: 0.2890 - val_loss: 0.3301 - val_brownian_loss: 0.3348 - val_sigma_loss: 0.2884\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3081 - brownian_loss: 0.3125 - sigma_loss: 0.2680 - val_loss: 0.3190 - val_brownian_loss: 0.3253 - val_sigma_loss: 0.2620\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3008 - brownian_loss: 0.3046 - sigma_loss: 0.2668 - val_loss: 0.3354 - val_brownian_loss: 0.3397 - val_sigma_loss: 0.2965\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3230 - brownian_loss: 0.3273 - sigma_loss: 0.2841 - val_loss: 0.3510 - val_brownian_loss: 0.3532 - val_sigma_loss: 0.3312\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3303 - brownian_loss: 0.3341 - sigma_loss: 0.2954 - val_loss: 0.3125 - val_brownian_loss: 0.3186 - val_sigma_loss: 0.2581\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3132 - brownian_loss: 0.3177 - sigma_loss: 0.2731 - val_loss: 0.3611 - val_brownian_loss: 0.3645 - val_sigma_loss: 0.3304\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3122 - brownian_loss: 0.3160 - sigma_loss: 0.2786 - val_loss: 0.3694 - val_brownian_loss: 0.3732 - val_sigma_loss: 0.3349\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3242 - brownian_loss: 0.3288 - sigma_loss: 0.2828 - val_loss: 0.3200 - val_brownian_loss: 0.3216 - val_sigma_loss: 0.3055\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 151s - loss: 0.2970 - brownian_loss: 0.3009 - sigma_loss: 0.2618 - val_loss: 0.3405 - val_brownian_loss: 0.3461 - val_sigma_loss: 0.2898\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3271 - brownian_loss: 0.3320 - sigma_loss: 0.2834 - val_loss: 0.3448 - val_brownian_loss: 0.3469 - val_sigma_loss: 0.3258\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3129 - brownian_loss: 0.3175 - sigma_loss: 0.2709 - val_loss: 0.3035 - val_brownian_loss: 0.3087 - val_sigma_loss: 0.2567\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3107 - brownian_loss: 0.3147 - sigma_loss: 0.2753 - val_loss: 0.2982 - val_brownian_loss: 0.3013 - val_sigma_loss: 0.2707\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3184 - brownian_loss: 0.3231 - sigma_loss: 0.2761 - val_loss: 0.2913 - val_brownian_loss: 0.2978 - val_sigma_loss: 0.2326\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3029 - brownian_loss: 0.3070 - sigma_loss: 0.2660 - val_loss: 0.3152 - val_brownian_loss: 0.3219 - val_sigma_loss: 0.2550\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3204 - brownian_loss: 0.3245 - sigma_loss: 0.2833 - val_loss: 0.3311 - val_brownian_loss: 0.3338 - val_sigma_loss: 0.3070\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3134 - brownian_loss: 0.3175 - sigma_loss: 0.2767 - val_loss: 0.2880 - val_brownian_loss: 0.2949 - val_sigma_loss: 0.2261\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3049 - brownian_loss: 0.3087 - sigma_loss: 0.2703 - val_loss: 0.3136 - val_brownian_loss: 0.3188 - val_sigma_loss: 0.2665\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3317 - brownian_loss: 0.3361 - sigma_loss: 0.2913 - val_loss: 0.3425 - val_brownian_loss: 0.3474 - val_sigma_loss: 0.2980\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3066 - brownian_loss: 0.3103 - sigma_loss: 0.2738 - val_loss: 0.2968 - val_brownian_loss: 0.3013 - val_sigma_loss: 0.2563\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3255 - brownian_loss: 0.3299 - sigma_loss: 0.2856 - val_loss: 0.2926 - val_brownian_loss: 0.2974 - val_sigma_loss: 0.2495\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3100 - brownian_loss: 0.3142 - sigma_loss: 0.2721 - val_loss: 0.2963 - val_brownian_loss: 0.2991 - val_sigma_loss: 0.2711\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 118s - loss: 0.3061 - brownian_loss: 0.3099 - sigma_loss: 0.2719 - val_loss: 0.2980 - val_brownian_loss: 0.3032 - val_sigma_loss: 0.2511\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3237 - brownian_loss: 0.3281 - sigma_loss: 0.2845 - val_loss: 0.2917 - val_brownian_loss: 0.2955 - val_sigma_loss: 0.2577\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 152s - loss: 0.3080 - brownian_loss: 0.3116 - sigma_loss: 0.2756 - val_loss: 0.2980 - val_brownian_loss: 0.2982 - val_sigma_loss: 0.2968\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3229 - brownian_loss: 0.3274 - sigma_loss: 0.2829 - val_loss: 0.3343 - val_brownian_loss: 0.3391 - val_sigma_loss: 0.2910\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.3186 - brownian_loss: 0.3233 - sigma_loss: 0.2766 - val_loss: 0.3337 - val_brownian_loss: 0.3401 - val_sigma_loss: 0.2761\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3075 - brownian_loss: 0.3112 - sigma_loss: 0.2740 - val_loss: 0.3051 - val_brownian_loss: 0.3111 - val_sigma_loss: 0.2516\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3210 - brownian_loss: 0.3245 - sigma_loss: 0.2897 - val_loss: 0.3447 - val_brownian_loss: 0.3471 - val_sigma_loss: 0.3235\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3071 - brownian_loss: 0.3108 - sigma_loss: 0.2737 - val_loss: 0.3066 - val_brownian_loss: 0.3126 - val_sigma_loss: 0.2524\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3234 - brownian_loss: 0.3277 - sigma_loss: 0.2850 - val_loss: 0.3172 - val_brownian_loss: 0.3191 - val_sigma_loss: 0.2999\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3121 - brownian_loss: 0.3169 - sigma_loss: 0.2691 - val_loss: 0.3152 - val_brownian_loss: 0.3205 - val_sigma_loss: 0.2673\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3101 - brownian_loss: 0.3142 - sigma_loss: 0.2729 - val_loss: 0.3567 - val_brownian_loss: 0.3593 - val_sigma_loss: 0.3332\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3285 - brownian_loss: 0.3325 - sigma_loss: 0.2923 - val_loss: 0.3327 - val_brownian_loss: 0.3400 - val_sigma_loss: 0.2668\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 150s - loss: 0.3055 - brownian_loss: 0.3091 - sigma_loss: 0.2729 - val_loss: 0.2982 - val_brownian_loss: 0.3005 - val_sigma_loss: 0.2778\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3174 - brownian_loss: 0.3216 - sigma_loss: 0.2797 - val_loss: 0.3032 - val_brownian_loss: 0.3074 - val_sigma_loss: 0.2656\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 79s - loss: 0.3152 - brownian_loss: 0.3193 - sigma_loss: 0.2785 - val_loss: 0.2990 - val_brownian_loss: 0.3019 - val_sigma_loss: 0.2734\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.3005 - brownian_loss: 0.3046 - sigma_loss: 0.2639 - val_loss: 0.2994 - val_brownian_loss: 0.3012 - val_sigma_loss: 0.2835\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3207 - brownian_loss: 0.3251 - sigma_loss: 0.2808 - val_loss: 0.3514 - val_brownian_loss: 0.3502 - val_sigma_loss: 0.3625\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.2992 - brownian_loss: 0.3033 - sigma_loss: 0.2624 - val_loss: 0.3824 - val_brownian_loss: 0.3883 - val_sigma_loss: 0.3297\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3171 - brownian_loss: 0.3209 - sigma_loss: 0.2831 - val_loss: 0.3036 - val_brownian_loss: 0.3064 - val_sigma_loss: 0.2782\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3076 - brownian_loss: 0.3114 - sigma_loss: 0.2729 - val_loss: 0.3075 - val_brownian_loss: 0.3142 - val_sigma_loss: 0.2471\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3109 - brownian_loss: 0.3149 - sigma_loss: 0.2747 - val_loss: 0.3151 - val_brownian_loss: 0.3187 - val_sigma_loss: 0.2826\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3233 - brownian_loss: 0.3278 - sigma_loss: 0.2825 - val_loss: 0.3516 - val_brownian_loss: 0.3599 - val_sigma_loss: 0.2770\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 149s - loss: 0.3020 - brownian_loss: 0.3065 - sigma_loss: 0.2622 - val_loss: 0.3049 - val_brownian_loss: 0.3090 - val_sigma_loss: 0.2675\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3200 - brownian_loss: 0.3237 - sigma_loss: 0.2871 - val_loss: 0.3230 - val_brownian_loss: 0.3255 - val_sigma_loss: 0.3009\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.2939 - brownian_loss: 0.2979 - sigma_loss: 0.2572 - val_loss: 0.3460 - val_brownian_loss: 0.3497 - val_sigma_loss: 0.3126\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3042 - brownian_loss: 0.3079 - sigma_loss: 0.2708 - val_loss: 0.3039 - val_brownian_loss: 0.3103 - val_sigma_loss: 0.2458\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3228 - brownian_loss: 0.3275 - sigma_loss: 0.2811 - val_loss: 0.3165 - val_brownian_loss: 0.3209 - val_sigma_loss: 0.2768\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 148s - loss: 0.3023 - brownian_loss: 0.3068 - sigma_loss: 0.2619 - val_loss: 0.3025 - val_brownian_loss: 0.3076 - val_sigma_loss: 0.2573\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3243 - brownian_loss: 0.3287 - sigma_loss: 0.2849 - val_loss: 0.3166 - val_brownian_loss: 0.3197 - val_sigma_loss: 0.2892\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3075 - brownian_loss: 0.3118 - sigma_loss: 0.2691 - val_loss: 0.2973 - val_brownian_loss: 0.3017 - val_sigma_loss: 0.2576\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 117s - loss: 0.3042 - brownian_loss: 0.3080 - sigma_loss: 0.2703 - val_loss: 0.3116 - val_brownian_loss: 0.3152 - val_sigma_loss: 0.2788\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3226 - brownian_loss: 0.3269 - sigma_loss: 0.2845 - val_loss: 0.3172 - val_brownian_loss: 0.3214 - val_sigma_loss: 0.2792\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.3060 - brownian_loss: 0.3101 - sigma_loss: 0.2691 - val_loss: 0.2943 - val_brownian_loss: 0.2994 - val_sigma_loss: 0.2483\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 41s - loss: 0.3177 - brownian_loss: 0.3220 - sigma_loss: 0.2787 - val_loss: 0.3537 - val_brownian_loss: 0.3565 - val_sigma_loss: 0.3285\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3123 - brownian_loss: 0.3174 - sigma_loss: 0.2669 - val_loss: 0.3371 - val_brownian_loss: 0.3388 - val_sigma_loss: 0.3215\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 116s - loss: 0.3057 - brownian_loss: 0.3095 - sigma_loss: 0.2716 - val_loss: 0.2697 - val_brownian_loss: 0.2755 - val_sigma_loss: 0.2167\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3249 - brownian_loss: 0.3291 - sigma_loss: 0.2870 - val_loss: 0.3026 - val_brownian_loss: 0.3108 - val_sigma_loss: 0.2289\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 147s - loss: 0.3003 - brownian_loss: 0.3038 - sigma_loss: 0.2684 - val_loss: 0.2768 - val_brownian_loss: 0.2814 - val_sigma_loss: 0.2356\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3195 - brownian_loss: 0.3240 - sigma_loss: 0.2788 - val_loss: 0.2721 - val_brownian_loss: 0.2747 - val_sigma_loss: 0.2483\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3068 - brownian_loss: 0.3119 - sigma_loss: 0.2610 - val_loss: 0.3015 - val_brownian_loss: 0.3074 - val_sigma_loss: 0.2483\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3060 - brownian_loss: 0.3103 - sigma_loss: 0.2674 - val_loss: 0.3180 - val_brownian_loss: 0.3223 - val_sigma_loss: 0.2789\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3263 - brownian_loss: 0.3301 - sigma_loss: 0.2919 - val_loss: 0.3417 - val_brownian_loss: 0.3456 - val_sigma_loss: 0.3067\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 139s - loss: 0.2947 - brownian_loss: 0.2995 - sigma_loss: 0.2519 - val_loss: 0.2824 - val_brownian_loss: 0.2869 - val_sigma_loss: 0.2421\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3261 - brownian_loss: 0.3305 - sigma_loss: 0.2869 - val_loss: 0.3237 - val_brownian_loss: 0.3305 - val_sigma_loss: 0.2631\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3138 - brownian_loss: 0.3187 - sigma_loss: 0.2697 - val_loss: 0.3214 - val_brownian_loss: 0.3272 - val_sigma_loss: 0.2698\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.3052 - brownian_loss: 0.3092 - sigma_loss: 0.2692 - val_loss: 0.2779 - val_brownian_loss: 0.2799 - val_sigma_loss: 0.2601\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3298 - brownian_loss: 0.3335 - sigma_loss: 0.2965 - val_loss: 0.3162 - val_brownian_loss: 0.3173 - val_sigma_loss: 0.3058\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.3028 - brownian_loss: 0.3068 - sigma_loss: 0.2665 - val_loss: 0.3136 - val_brownian_loss: 0.3171 - val_sigma_loss: 0.2823\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3149 - brownian_loss: 0.3187 - sigma_loss: 0.2802 - val_loss: 0.3225 - val_brownian_loss: 0.3256 - val_sigma_loss: 0.2939\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3060 - brownian_loss: 0.3110 - sigma_loss: 0.2615 - val_loss: 0.2956 - val_brownian_loss: 0.3013 - val_sigma_loss: 0.2440\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.3047 - brownian_loss: 0.3075 - sigma_loss: 0.2795 - val_loss: 0.3272 - val_brownian_loss: 0.3309 - val_sigma_loss: 0.2939\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3216 - brownian_loss: 0.3255 - sigma_loss: 0.2862 - val_loss: 0.3170 - val_brownian_loss: 0.3206 - val_sigma_loss: 0.2847\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2960 - brownian_loss: 0.2999 - sigma_loss: 0.2605 - val_loss: 0.2955 - val_brownian_loss: 0.3006 - val_sigma_loss: 0.2498\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3188 - brownian_loss: 0.3228 - sigma_loss: 0.2828 - val_loss: 0.3346 - val_brownian_loss: 0.3380 - val_sigma_loss: 0.3040\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3126 - brownian_loss: 0.3175 - sigma_loss: 0.2679 - val_loss: 0.3039 - val_brownian_loss: 0.3090 - val_sigma_loss: 0.2573\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2923 - brownian_loss: 0.2959 - sigma_loss: 0.2603 - val_loss: 0.3029 - val_brownian_loss: 0.3064 - val_sigma_loss: 0.2716\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3232 - brownian_loss: 0.3274 - sigma_loss: 0.2855 - val_loss: 0.3222 - val_brownian_loss: 0.3255 - val_sigma_loss: 0.2919\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2945 - brownian_loss: 0.2986 - sigma_loss: 0.2581 - val_loss: 0.2949 - val_brownian_loss: 0.2959 - val_sigma_loss: 0.2862\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3228 - brownian_loss: 0.3267 - sigma_loss: 0.2870 - val_loss: 0.2844 - val_brownian_loss: 0.2886 - val_sigma_loss: 0.2462\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3038 - brownian_loss: 0.3088 - sigma_loss: 0.2587 - val_loss: 0.3652 - val_brownian_loss: 0.3700 - val_sigma_loss: 0.3224\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.3036 - brownian_loss: 0.3081 - sigma_loss: 0.2632 - val_loss: 0.3410 - val_brownian_loss: 0.3448 - val_sigma_loss: 0.3064\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3222 - brownian_loss: 0.3262 - sigma_loss: 0.2864 - val_loss: 0.3357 - val_brownian_loss: 0.3427 - val_sigma_loss: 0.2723\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 139s - loss: 0.2927 - brownian_loss: 0.2969 - sigma_loss: 0.2552 - val_loss: 0.2961 - val_brownian_loss: 0.2984 - val_sigma_loss: 0.2759\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3196 - brownian_loss: 0.3236 - sigma_loss: 0.2840 - val_loss: 0.3331 - val_brownian_loss: 0.3377 - val_sigma_loss: 0.2912\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3003 - brownian_loss: 0.3054 - sigma_loss: 0.2548 - val_loss: 0.2919 - val_brownian_loss: 0.2952 - val_sigma_loss: 0.2624\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 108s - loss: 0.3007 - brownian_loss: 0.3053 - sigma_loss: 0.2594 - val_loss: 0.3050 - val_brownian_loss: 0.3094 - val_sigma_loss: 0.2646\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3239 - brownian_loss: 0.3276 - sigma_loss: 0.2907 - val_loss: 0.3445 - val_brownian_loss: 0.3476 - val_sigma_loss: 0.3169\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2995 - brownian_loss: 0.3032 - sigma_loss: 0.2668 - val_loss: 0.2752 - val_brownian_loss: 0.2811 - val_sigma_loss: 0.2220\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3247 - brownian_loss: 0.3291 - sigma_loss: 0.2853 - val_loss: 0.3362 - val_brownian_loss: 0.3408 - val_sigma_loss: 0.2953\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3013 - brownian_loss: 0.3061 - sigma_loss: 0.2579 - val_loss: 0.3061 - val_brownian_loss: 0.3107 - val_sigma_loss: 0.2646\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.3023 - brownian_loss: 0.3068 - sigma_loss: 0.2619 - val_loss: 0.2893 - val_brownian_loss: 0.2941 - val_sigma_loss: 0.2456\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3160 - brownian_loss: 0.3194 - sigma_loss: 0.2849 - val_loss: 0.2959 - val_brownian_loss: 0.3001 - val_sigma_loss: 0.2579\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2952 - brownian_loss: 0.2996 - sigma_loss: 0.2547 - val_loss: 0.2731 - val_brownian_loss: 0.2783 - val_sigma_loss: 0.2263\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3241 - brownian_loss: 0.3284 - sigma_loss: 0.2856 - val_loss: 0.3145 - val_brownian_loss: 0.3163 - val_sigma_loss: 0.2987\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 70s - loss: 0.3088 - brownian_loss: 0.3135 - sigma_loss: 0.2662 - val_loss: 0.2966 - val_brownian_loss: 0.3042 - val_sigma_loss: 0.2286\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.3008 - brownian_loss: 0.3046 - sigma_loss: 0.2660 - val_loss: 0.3050 - val_brownian_loss: 0.3100 - val_sigma_loss: 0.2604\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3182 - brownian_loss: 0.3224 - sigma_loss: 0.2802 - val_loss: 0.3495 - val_brownian_loss: 0.3547 - val_sigma_loss: 0.3024\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2950 - brownian_loss: 0.2989 - sigma_loss: 0.2593 - val_loss: 0.3230 - val_brownian_loss: 0.3290 - val_sigma_loss: 0.2693\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3150 - brownian_loss: 0.3187 - sigma_loss: 0.2813 - val_loss: 0.3433 - val_brownian_loss: 0.3512 - val_sigma_loss: 0.2718\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3043 - brownian_loss: 0.3088 - sigma_loss: 0.2637 - val_loss: 0.3316 - val_brownian_loss: 0.3364 - val_sigma_loss: 0.2877\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2954 - brownian_loss: 0.2994 - sigma_loss: 0.2594 - val_loss: 0.3055 - val_brownian_loss: 0.3080 - val_sigma_loss: 0.2834\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3300 - brownian_loss: 0.3344 - sigma_loss: 0.2900 - val_loss: 0.3329 - val_brownian_loss: 0.3360 - val_sigma_loss: 0.3042\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2929 - brownian_loss: 0.2970 - sigma_loss: 0.2560 - val_loss: 0.3213 - val_brownian_loss: 0.3232 - val_sigma_loss: 0.3039\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3175 - brownian_loss: 0.3219 - sigma_loss: 0.2773 - val_loss: 0.3173 - val_brownian_loss: 0.3192 - val_sigma_loss: 0.3000\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3070 - brownian_loss: 0.3113 - sigma_loss: 0.2682 - val_loss: 0.2970 - val_brownian_loss: 0.3022 - val_sigma_loss: 0.2497\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2926 - brownian_loss: 0.2967 - sigma_loss: 0.2560 - val_loss: 0.3202 - val_brownian_loss: 0.3233 - val_sigma_loss: 0.2921\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3193 - brownian_loss: 0.3235 - sigma_loss: 0.2815 - val_loss: 0.3607 - val_brownian_loss: 0.3671 - val_sigma_loss: 0.3032\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2945 - brownian_loss: 0.2983 - sigma_loss: 0.2604 - val_loss: 0.2805 - val_brownian_loss: 0.2848 - val_sigma_loss: 0.2420\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3185 - brownian_loss: 0.3232 - sigma_loss: 0.2767 - val_loss: 0.3063 - val_brownian_loss: 0.3097 - val_sigma_loss: 0.2757\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2947 - brownian_loss: 0.2992 - sigma_loss: 0.2548 - val_loss: 0.3140 - val_brownian_loss: 0.3182 - val_sigma_loss: 0.2765\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2888 - brownian_loss: 0.2926 - sigma_loss: 0.2549 - val_loss: 0.3054 - val_brownian_loss: 0.3091 - val_sigma_loss: 0.2714\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3167 - brownian_loss: 0.3211 - sigma_loss: 0.2774 - val_loss: 0.3447 - val_brownian_loss: 0.3481 - val_sigma_loss: 0.3142\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2932 - brownian_loss: 0.2976 - sigma_loss: 0.2532 - val_loss: 0.2862 - val_brownian_loss: 0.2927 - val_sigma_loss: 0.2278\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3222 - brownian_loss: 0.3270 - sigma_loss: 0.2786 - val_loss: 0.2849 - val_brownian_loss: 0.2884 - val_sigma_loss: 0.2533\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.3113 - brownian_loss: 0.3159 - sigma_loss: 0.2702 - val_loss: 0.2923 - val_brownian_loss: 0.2958 - val_sigma_loss: 0.2605\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2905 - brownian_loss: 0.2944 - sigma_loss: 0.2547 - val_loss: 0.3120 - val_brownian_loss: 0.3167 - val_sigma_loss: 0.2701\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3251 - brownian_loss: 0.3296 - sigma_loss: 0.2843 - val_loss: 0.3076 - val_brownian_loss: 0.3125 - val_sigma_loss: 0.2635\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2982 - brownian_loss: 0.3017 - sigma_loss: 0.2670 - val_loss: 0.2723 - val_brownian_loss: 0.2753 - val_sigma_loss: 0.2448\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3213 - brownian_loss: 0.3245 - sigma_loss: 0.2925 - val_loss: 0.2898 - val_brownian_loss: 0.2930 - val_sigma_loss: 0.2610\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.3063 - brownian_loss: 0.3103 - sigma_loss: 0.2695 - val_loss: 0.2773 - val_brownian_loss: 0.2860 - val_sigma_loss: 0.1993\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2934 - brownian_loss: 0.2976 - sigma_loss: 0.2553 - val_loss: 0.2877 - val_brownian_loss: 0.2923 - val_sigma_loss: 0.2465\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3168 - brownian_loss: 0.3206 - sigma_loss: 0.2825 - val_loss: 0.3110 - val_brownian_loss: 0.3161 - val_sigma_loss: 0.2648\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2957 - brownian_loss: 0.2995 - sigma_loss: 0.2611 - val_loss: 0.3073 - val_brownian_loss: 0.3096 - val_sigma_loss: 0.2871\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3126 - brownian_loss: 0.3165 - sigma_loss: 0.2779 - val_loss: 0.3154 - val_brownian_loss: 0.3194 - val_sigma_loss: 0.2800\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3060 - brownian_loss: 0.3103 - sigma_loss: 0.2673 - val_loss: 0.2991 - val_brownian_loss: 0.3047 - val_sigma_loss: 0.2492\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.3018 - brownian_loss: 0.3060 - sigma_loss: 0.2645 - val_loss: 0.3158 - val_brownian_loss: 0.3198 - val_sigma_loss: 0.2795\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3263 - brownian_loss: 0.3310 - sigma_loss: 0.2833 - val_loss: 0.3491 - val_brownian_loss: 0.3568 - val_sigma_loss: 0.2806\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2976 - brownian_loss: 0.3019 - sigma_loss: 0.2591 - val_loss: 0.2926 - val_brownian_loss: 0.2956 - val_sigma_loss: 0.2655\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3127 - brownian_loss: 0.3167 - sigma_loss: 0.2766 - val_loss: 0.3229 - val_brownian_loss: 0.3280 - val_sigma_loss: 0.2775\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2953 - brownian_loss: 0.2999 - sigma_loss: 0.2545 - val_loss: 0.3294 - val_brownian_loss: 0.3327 - val_sigma_loss: 0.3004\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 115s - loss: 0.2936 - brownian_loss: 0.2979 - sigma_loss: 0.2546 - val_loss: 0.2894 - val_brownian_loss: 0.2939 - val_sigma_loss: 0.2487\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3277 - brownian_loss: 0.3324 - sigma_loss: 0.2862 - val_loss: 0.2711 - val_brownian_loss: 0.2741 - val_sigma_loss: 0.2442\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2945 - brownian_loss: 0.2983 - sigma_loss: 0.2603 - val_loss: 0.3053 - val_brownian_loss: 0.3101 - val_sigma_loss: 0.2619\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3100 - brownian_loss: 0.3140 - sigma_loss: 0.2739 - val_loss: 0.3314 - val_brownian_loss: 0.3361 - val_sigma_loss: 0.2890\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3072 - brownian_loss: 0.3115 - sigma_loss: 0.2688 - val_loss: 0.3425 - val_brownian_loss: 0.3503 - val_sigma_loss: 0.2724\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2837 - brownian_loss: 0.2873 - sigma_loss: 0.2512 - val_loss: 0.2956 - val_brownian_loss: 0.2989 - val_sigma_loss: 0.2659\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3179 - brownian_loss: 0.3218 - sigma_loss: 0.2819 - val_loss: 0.3296 - val_brownian_loss: 0.3338 - val_sigma_loss: 0.2915\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2922 - brownian_loss: 0.2961 - sigma_loss: 0.2567 - val_loss: 0.2990 - val_brownian_loss: 0.3002 - val_sigma_loss: 0.2877\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3154 - brownian_loss: 0.3196 - sigma_loss: 0.2769 - val_loss: 0.3261 - val_brownian_loss: 0.3313 - val_sigma_loss: 0.2789\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3029 - brownian_loss: 0.3080 - sigma_loss: 0.2566 - val_loss: 0.3031 - val_brownian_loss: 0.3093 - val_sigma_loss: 0.2466\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.2964 - brownian_loss: 0.3006 - sigma_loss: 0.2586 - val_loss: 0.3093 - val_brownian_loss: 0.3163 - val_sigma_loss: 0.2470\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3186 - brownian_loss: 0.3232 - sigma_loss: 0.2769 - val_loss: 0.2880 - val_brownian_loss: 0.2902 - val_sigma_loss: 0.2681\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2875 - brownian_loss: 0.2915 - sigma_loss: 0.2518 - val_loss: 0.2950 - val_brownian_loss: 0.2995 - val_sigma_loss: 0.2545\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3176 - brownian_loss: 0.3214 - sigma_loss: 0.2828 - val_loss: 0.3216 - val_brownian_loss: 0.3284 - val_sigma_loss: 0.2603\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2944 - brownian_loss: 0.2986 - sigma_loss: 0.2567 - val_loss: 0.3140 - val_brownian_loss: 0.3207 - val_sigma_loss: 0.2532\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2944 - brownian_loss: 0.2984 - sigma_loss: 0.2589 - val_loss: 0.2846 - val_brownian_loss: 0.2871 - val_sigma_loss: 0.2626\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3209 - brownian_loss: 0.3254 - sigma_loss: 0.2808 - val_loss: 0.2720 - val_brownian_loss: 0.2745 - val_sigma_loss: 0.2499\n",
      "50\n",
      "1.0\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3233 - brownian_loss: 0.3275 - sigma_loss: 0.2855 - val_loss: 0.3441 - val_brownian_loss: 0.3468 - val_sigma_loss: 0.3194\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3145 - brownian_loss: 0.3188 - sigma_loss: 0.2760 - val_loss: 0.2780 - val_brownian_loss: 0.2812 - val_sigma_loss: 0.2493\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2977 - brownian_loss: 0.3022 - sigma_loss: 0.2572 - val_loss: 0.2667 - val_brownian_loss: 0.2713 - val_sigma_loss: 0.2256\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3262 - brownian_loss: 0.3301 - sigma_loss: 0.2911 - val_loss: 0.3108 - val_brownian_loss: 0.3155 - val_sigma_loss: 0.2689\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2868 - brownian_loss: 0.2909 - sigma_loss: 0.2501 - val_loss: 0.2913 - val_brownian_loss: 0.2955 - val_sigma_loss: 0.2537\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3163 - brownian_loss: 0.3213 - sigma_loss: 0.2716 - val_loss: 0.3134 - val_brownian_loss: 0.3192 - val_sigma_loss: 0.2609\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2898 - brownian_loss: 0.2949 - sigma_loss: 0.2447 - val_loss: 0.2915 - val_brownian_loss: 0.2972 - val_sigma_loss: 0.2402\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2904 - brownian_loss: 0.2951 - sigma_loss: 0.2481 - val_loss: 0.3067 - val_brownian_loss: 0.3101 - val_sigma_loss: 0.2764\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3227 - brownian_loss: 0.3272 - sigma_loss: 0.2822 - val_loss: 0.2975 - val_brownian_loss: 0.3011 - val_sigma_loss: 0.2651\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2906 - brownian_loss: 0.2954 - sigma_loss: 0.2469 - val_loss: 0.2977 - val_brownian_loss: 0.3001 - val_sigma_loss: 0.2761\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3096 - brownian_loss: 0.3141 - sigma_loss: 0.2692 - val_loss: 0.3347 - val_brownian_loss: 0.3375 - val_sigma_loss: 0.3092\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2971 - brownian_loss: 0.3018 - sigma_loss: 0.2547 - val_loss: 0.2753 - val_brownian_loss: 0.2818 - val_sigma_loss: 0.2163\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 108s - loss: 0.2955 - brownian_loss: 0.2997 - sigma_loss: 0.2577 - val_loss: 0.2785 - val_brownian_loss: 0.2841 - val_sigma_loss: 0.2284\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3125 - brownian_loss: 0.3169 - sigma_loss: 0.2736 - val_loss: 0.3265 - val_brownian_loss: 0.3335 - val_sigma_loss: 0.2637\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2840 - brownian_loss: 0.2876 - sigma_loss: 0.2519 - val_loss: 0.2804 - val_brownian_loss: 0.2871 - val_sigma_loss: 0.2207\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3137 - brownian_loss: 0.3176 - sigma_loss: 0.2783 - val_loss: 0.3033 - val_brownian_loss: 0.3073 - val_sigma_loss: 0.2668\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2928 - brownian_loss: 0.2969 - sigma_loss: 0.2551 - val_loss: 0.3063 - val_brownian_loss: 0.3087 - val_sigma_loss: 0.2850\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2815 - brownian_loss: 0.2854 - sigma_loss: 0.2465 - val_loss: 0.2704 - val_brownian_loss: 0.2753 - val_sigma_loss: 0.2262\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3206 - brownian_loss: 0.3245 - sigma_loss: 0.2852 - val_loss: 0.3227 - val_brownian_loss: 0.3279 - val_sigma_loss: 0.2754\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2851 - brownian_loss: 0.2893 - sigma_loss: 0.2473 - val_loss: 0.2655 - val_brownian_loss: 0.2710 - val_sigma_loss: 0.2160\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3118 - brownian_loss: 0.3154 - sigma_loss: 0.2793 - val_loss: 0.2757 - val_brownian_loss: 0.2800 - val_sigma_loss: 0.2366\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2929 - brownian_loss: 0.2980 - sigma_loss: 0.2470 - val_loss: 0.2975 - val_brownian_loss: 0.3020 - val_sigma_loss: 0.2576\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2906 - brownian_loss: 0.2949 - sigma_loss: 0.2519 - val_loss: 0.2794 - val_brownian_loss: 0.2818 - val_sigma_loss: 0.2577\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3187 - brownian_loss: 0.3225 - sigma_loss: 0.2845 - val_loss: 0.2787 - val_brownian_loss: 0.2816 - val_sigma_loss: 0.2521\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2876 - brownian_loss: 0.2920 - sigma_loss: 0.2482 - val_loss: 0.2722 - val_brownian_loss: 0.2770 - val_sigma_loss: 0.2291\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3133 - brownian_loss: 0.3174 - sigma_loss: 0.2763 - val_loss: 0.3545 - val_brownian_loss: 0.3583 - val_sigma_loss: 0.3206\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2992 - brownian_loss: 0.3041 - sigma_loss: 0.2549 - val_loss: 0.3455 - val_brownian_loss: 0.3468 - val_sigma_loss: 0.3336\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2827 - brownian_loss: 0.2866 - sigma_loss: 0.2477 - val_loss: 0.2766 - val_brownian_loss: 0.2817 - val_sigma_loss: 0.2304\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3242 - brownian_loss: 0.3283 - sigma_loss: 0.2877 - val_loss: 0.3280 - val_brownian_loss: 0.3335 - val_sigma_loss: 0.2779\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2888 - brownian_loss: 0.2928 - sigma_loss: 0.2527 - val_loss: 0.2864 - val_brownian_loss: 0.2917 - val_sigma_loss: 0.2393\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3118 - brownian_loss: 0.3160 - sigma_loss: 0.2737 - val_loss: 0.3071 - val_brownian_loss: 0.3110 - val_sigma_loss: 0.2720\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2985 - brownian_loss: 0.3028 - sigma_loss: 0.2598 - val_loss: 0.3141 - val_brownian_loss: 0.3196 - val_sigma_loss: 0.2646\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2852 - brownian_loss: 0.2894 - sigma_loss: 0.2470 - val_loss: 0.2665 - val_brownian_loss: 0.2671 - val_sigma_loss: 0.2613\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3238 - brownian_loss: 0.3283 - sigma_loss: 0.2835 - val_loss: 0.3325 - val_brownian_loss: 0.3320 - val_sigma_loss: 0.3363\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2845 - brownian_loss: 0.2891 - sigma_loss: 0.2434 - val_loss: 0.2857 - val_brownian_loss: 0.2910 - val_sigma_loss: 0.2385\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3209 - brownian_loss: 0.3249 - sigma_loss: 0.2843 - val_loss: 0.3566 - val_brownian_loss: 0.3630 - val_sigma_loss: 0.2989\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2951 - brownian_loss: 0.3000 - sigma_loss: 0.2511 - val_loss: 0.3046 - val_brownian_loss: 0.3083 - val_sigma_loss: 0.2714\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2901 - brownian_loss: 0.2943 - sigma_loss: 0.2528 - val_loss: 0.2848 - val_brownian_loss: 0.2881 - val_sigma_loss: 0.2552\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3270 - brownian_loss: 0.3310 - sigma_loss: 0.2917 - val_loss: 0.3476 - val_brownian_loss: 0.3535 - val_sigma_loss: 0.2944\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2785 - brownian_loss: 0.2835 - sigma_loss: 0.2336 - val_loss: 0.3138 - val_brownian_loss: 0.3163 - val_sigma_loss: 0.2909\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3095 - brownian_loss: 0.3136 - sigma_loss: 0.2733 - val_loss: 0.3031 - val_brownian_loss: 0.3075 - val_sigma_loss: 0.2626\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2965 - brownian_loss: 0.3009 - sigma_loss: 0.2564 - val_loss: 0.2918 - val_brownian_loss: 0.2986 - val_sigma_loss: 0.2301\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2856 - brownian_loss: 0.2897 - sigma_loss: 0.2492 - val_loss: 0.3114 - val_brownian_loss: 0.3153 - val_sigma_loss: 0.2766\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3192 - brownian_loss: 0.3232 - sigma_loss: 0.2831 - val_loss: 0.3363 - val_brownian_loss: 0.3415 - val_sigma_loss: 0.2893\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2899 - brownian_loss: 0.2941 - sigma_loss: 0.2513 - val_loss: 0.2984 - val_brownian_loss: 0.3013 - val_sigma_loss: 0.2716\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3099 - brownian_loss: 0.3140 - sigma_loss: 0.2727 - val_loss: 0.3107 - val_brownian_loss: 0.3128 - val_sigma_loss: 0.2909\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3029 - brownian_loss: 0.3074 - sigma_loss: 0.2629 - val_loss: 0.3216 - val_brownian_loss: 0.3254 - val_sigma_loss: 0.2879\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2924 - brownian_loss: 0.2961 - sigma_loss: 0.2589 - val_loss: 0.3309 - val_brownian_loss: 0.3350 - val_sigma_loss: 0.2943\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3129 - brownian_loss: 0.3172 - sigma_loss: 0.2747 - val_loss: 0.2840 - val_brownian_loss: 0.2855 - val_sigma_loss: 0.2707\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2869 - brownian_loss: 0.2909 - sigma_loss: 0.2511 - val_loss: 0.2838 - val_brownian_loss: 0.2910 - val_sigma_loss: 0.2190\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3165 - brownian_loss: 0.3207 - sigma_loss: 0.2787 - val_loss: 0.2964 - val_brownian_loss: 0.3005 - val_sigma_loss: 0.2595\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3072 - brownian_loss: 0.3120 - sigma_loss: 0.2646 - val_loss: 0.3151 - val_brownian_loss: 0.3206 - val_sigma_loss: 0.2661\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2975 - brownian_loss: 0.3020 - sigma_loss: 0.2568 - val_loss: 0.2949 - val_brownian_loss: 0.2992 - val_sigma_loss: 0.2555\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3152 - brownian_loss: 0.3193 - sigma_loss: 0.2785 - val_loss: 0.3346 - val_brownian_loss: 0.3393 - val_sigma_loss: 0.2929\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2881 - brownian_loss: 0.2928 - sigma_loss: 0.2459 - val_loss: 0.2813 - val_brownian_loss: 0.2881 - val_sigma_loss: 0.2208\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3158 - brownian_loss: 0.3192 - sigma_loss: 0.2846 - val_loss: 0.2944 - val_brownian_loss: 0.2999 - val_sigma_loss: 0.2449\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2946 - brownian_loss: 0.2999 - sigma_loss: 0.2466 - val_loss: 0.2845 - val_brownian_loss: 0.2877 - val_sigma_loss: 0.2555\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2939 - brownian_loss: 0.2981 - sigma_loss: 0.2564 - val_loss: 0.3148 - val_brownian_loss: 0.3186 - val_sigma_loss: 0.2813\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3228 - brownian_loss: 0.3276 - sigma_loss: 0.2791 - val_loss: 0.3303 - val_brownian_loss: 0.3344 - val_sigma_loss: 0.2935\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2884 - brownian_loss: 0.2929 - sigma_loss: 0.2475 - val_loss: 0.2850 - val_brownian_loss: 0.2885 - val_sigma_loss: 0.2531\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3115 - brownian_loss: 0.3159 - sigma_loss: 0.2721 - val_loss: 0.3276 - val_brownian_loss: 0.3331 - val_sigma_loss: 0.2784\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2969 - brownian_loss: 0.3019 - sigma_loss: 0.2515 - val_loss: 0.3387 - val_brownian_loss: 0.3439 - val_sigma_loss: 0.2917\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2961 - brownian_loss: 0.2998 - sigma_loss: 0.2628 - val_loss: 0.2501 - val_brownian_loss: 0.2568 - val_sigma_loss: 0.1892\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3175 - brownian_loss: 0.3215 - sigma_loss: 0.2820 - val_loss: 0.3237 - val_brownian_loss: 0.3268 - val_sigma_loss: 0.2962\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2894 - brownian_loss: 0.2938 - sigma_loss: 0.2499 - val_loss: 0.3082 - val_brownian_loss: 0.3119 - val_sigma_loss: 0.2749\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3126 - brownian_loss: 0.3171 - sigma_loss: 0.2719 - val_loss: 0.3054 - val_brownian_loss: 0.3098 - val_sigma_loss: 0.2658\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.3014 - brownian_loss: 0.3059 - sigma_loss: 0.2609 - val_loss: 0.3057 - val_brownian_loss: 0.3114 - val_sigma_loss: 0.2545\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.2947 - brownian_loss: 0.2987 - sigma_loss: 0.2586 - val_loss: 0.2642 - val_brownian_loss: 0.2691 - val_sigma_loss: 0.2197\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3210 - brownian_loss: 0.3251 - sigma_loss: 0.2843 - val_loss: 0.3463 - val_brownian_loss: 0.3487 - val_sigma_loss: 0.3248\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2840 - brownian_loss: 0.2884 - sigma_loss: 0.2445 - val_loss: 0.2879 - val_brownian_loss: 0.2911 - val_sigma_loss: 0.2587\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3073 - brownian_loss: 0.3115 - sigma_loss: 0.2696 - val_loss: 0.3104 - val_brownian_loss: 0.3151 - val_sigma_loss: 0.2674\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2980 - brownian_loss: 0.3024 - sigma_loss: 0.2577 - val_loss: 0.3423 - val_brownian_loss: 0.3496 - val_sigma_loss: 0.2764\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2884 - brownian_loss: 0.2926 - sigma_loss: 0.2500 - val_loss: 0.2630 - val_brownian_loss: 0.2676 - val_sigma_loss: 0.2216\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3185 - brownian_loss: 0.3232 - sigma_loss: 0.2765 - val_loss: 0.3018 - val_brownian_loss: 0.3086 - val_sigma_loss: 0.2403\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.2858 - brownian_loss: 0.2900 - sigma_loss: 0.2488 - val_loss: 0.3166 - val_brownian_loss: 0.3215 - val_sigma_loss: 0.2729\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3160 - brownian_loss: 0.3197 - sigma_loss: 0.2829 - val_loss: 0.3382 - val_brownian_loss: 0.3435 - val_sigma_loss: 0.2910\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2938 - brownian_loss: 0.2981 - sigma_loss: 0.2554 - val_loss: 0.3150 - val_brownian_loss: 0.3205 - val_sigma_loss: 0.2653\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3025 - brownian_loss: 0.3066 - sigma_loss: 0.2655 - val_loss: 0.2915 - val_brownian_loss: 0.2947 - val_sigma_loss: 0.2621\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3101 - brownian_loss: 0.3147 - sigma_loss: 0.2684 - val_loss: 0.3244 - val_brownian_loss: 0.3280 - val_sigma_loss: 0.2919\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2876 - brownian_loss: 0.2915 - sigma_loss: 0.2527 - val_loss: 0.3189 - val_brownian_loss: 0.3256 - val_sigma_loss: 0.2578\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3224 - brownian_loss: 0.3269 - sigma_loss: 0.2820 - val_loss: 0.3282 - val_brownian_loss: 0.3336 - val_sigma_loss: 0.2797\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2939 - brownian_loss: 0.2988 - sigma_loss: 0.2497 - val_loss: 0.2873 - val_brownian_loss: 0.2902 - val_sigma_loss: 0.2619\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.2854 - brownian_loss: 0.2901 - sigma_loss: 0.2433 - val_loss: 0.2729 - val_brownian_loss: 0.2774 - val_sigma_loss: 0.2320\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3169 - brownian_loss: 0.3214 - sigma_loss: 0.2770 - val_loss: 0.2994 - val_brownian_loss: 0.3026 - val_sigma_loss: 0.2706\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2855 - brownian_loss: 0.2902 - sigma_loss: 0.2434 - val_loss: 0.2830 - val_brownian_loss: 0.2891 - val_sigma_loss: 0.2283\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3102 - brownian_loss: 0.3142 - sigma_loss: 0.2744 - val_loss: 0.3234 - val_brownian_loss: 0.3287 - val_sigma_loss: 0.2762\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2888 - brownian_loss: 0.2937 - sigma_loss: 0.2448 - val_loss: 0.2959 - val_brownian_loss: 0.2999 - val_sigma_loss: 0.2599\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2881 - brownian_loss: 0.2920 - sigma_loss: 0.2538 - val_loss: 0.2896 - val_brownian_loss: 0.2936 - val_sigma_loss: 0.2536\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 24s - loss: 0.3158 - brownian_loss: 0.3201 - sigma_loss: 0.2776 - val_loss: 0.3265 - val_brownian_loss: 0.3271 - val_sigma_loss: 0.3217\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2905 - brownian_loss: 0.2948 - sigma_loss: 0.2521 - val_loss: 0.2820 - val_brownian_loss: 0.2873 - val_sigma_loss: 0.2346\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3115 - brownian_loss: 0.3162 - sigma_loss: 0.2691 - val_loss: 0.2691 - val_brownian_loss: 0.2731 - val_sigma_loss: 0.2333\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 76s - loss: 0.2968 - brownian_loss: 0.3019 - sigma_loss: 0.2515 - val_loss: 0.3046 - val_brownian_loss: 0.3093 - val_sigma_loss: 0.2624\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2841 - brownian_loss: 0.2874 - sigma_loss: 0.2545 - val_loss: 0.2820 - val_brownian_loss: 0.2867 - val_sigma_loss: 0.2404\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3185 - brownian_loss: 0.3229 - sigma_loss: 0.2786 - val_loss: 0.3139 - val_brownian_loss: 0.3158 - val_sigma_loss: 0.2967\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2896 - brownian_loss: 0.2942 - sigma_loss: 0.2482 - val_loss: 0.2835 - val_brownian_loss: 0.2851 - val_sigma_loss: 0.2685\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3115 - brownian_loss: 0.3157 - sigma_loss: 0.2730 - val_loss: 0.2957 - val_brownian_loss: 0.2985 - val_sigma_loss: 0.2706\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2964 - brownian_loss: 0.3009 - sigma_loss: 0.2553 - val_loss: 0.3140 - val_brownian_loss: 0.3179 - val_sigma_loss: 0.2785\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2946 - brownian_loss: 0.2990 - sigma_loss: 0.2548 - val_loss: 0.2774 - val_brownian_loss: 0.2835 - val_sigma_loss: 0.2232\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3161 - brownian_loss: 0.3201 - sigma_loss: 0.2799 - val_loss: 0.3139 - val_brownian_loss: 0.3193 - val_sigma_loss: 0.2652\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2739 - brownian_loss: 0.2781 - sigma_loss: 0.2369 - val_loss: 0.2927 - val_brownian_loss: 0.2967 - val_sigma_loss: 0.2562\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3186 - brownian_loss: 0.3227 - sigma_loss: 0.2821 - val_loss: 0.3444 - val_brownian_loss: 0.3475 - val_sigma_loss: 0.3164\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2941 - brownian_loss: 0.2984 - sigma_loss: 0.2554 - val_loss: 0.2843 - val_brownian_loss: 0.2905 - val_sigma_loss: 0.2290\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2920 - brownian_loss: 0.2961 - sigma_loss: 0.2545 - val_loss: 0.3208 - val_brownian_loss: 0.3213 - val_sigma_loss: 0.3158\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3183 - brownian_loss: 0.3227 - sigma_loss: 0.2788 - val_loss: 0.2978 - val_brownian_loss: 0.2957 - val_sigma_loss: 0.3163\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2759 - brownian_loss: 0.2804 - sigma_loss: 0.2356 - val_loss: 0.2467 - val_brownian_loss: 0.2507 - val_sigma_loss: 0.2101\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3071 - brownian_loss: 0.3115 - sigma_loss: 0.2671 - val_loss: 0.3392 - val_brownian_loss: 0.3443 - val_sigma_loss: 0.2936\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.3037 - brownian_loss: 0.3082 - sigma_loss: 0.2630 - val_loss: 0.2755 - val_brownian_loss: 0.2817 - val_sigma_loss: 0.2200\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2869 - brownian_loss: 0.2912 - sigma_loss: 0.2479 - val_loss: 0.2712 - val_brownian_loss: 0.2752 - val_sigma_loss: 0.2347\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3168 - brownian_loss: 0.3212 - sigma_loss: 0.2771 - val_loss: 0.3051 - val_brownian_loss: 0.3103 - val_sigma_loss: 0.2584\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2823 - brownian_loss: 0.2867 - sigma_loss: 0.2430 - val_loss: 0.3203 - val_brownian_loss: 0.3253 - val_sigma_loss: 0.2756\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3164 - brownian_loss: 0.3209 - sigma_loss: 0.2758 - val_loss: 0.3342 - val_brownian_loss: 0.3405 - val_sigma_loss: 0.2775\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3009 - brownian_loss: 0.3051 - sigma_loss: 0.2629 - val_loss: 0.3347 - val_brownian_loss: 0.3402 - val_sigma_loss: 0.2852\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2872 - brownian_loss: 0.2913 - sigma_loss: 0.2502 - val_loss: 0.2799 - val_brownian_loss: 0.2836 - val_sigma_loss: 0.2469\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3121 - brownian_loss: 0.3167 - sigma_loss: 0.2707 - val_loss: 0.3445 - val_brownian_loss: 0.3487 - val_sigma_loss: 0.3063\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2841 - brownian_loss: 0.2883 - sigma_loss: 0.2464 - val_loss: 0.2829 - val_brownian_loss: 0.2874 - val_sigma_loss: 0.2426\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3100 - brownian_loss: 0.3141 - sigma_loss: 0.2739 - val_loss: 0.3211 - val_brownian_loss: 0.3275 - val_sigma_loss: 0.2627\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3052 - brownian_loss: 0.3098 - sigma_loss: 0.2637 - val_loss: 0.2517 - val_brownian_loss: 0.2570 - val_sigma_loss: 0.2042\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 108s - loss: 0.2900 - brownian_loss: 0.2944 - sigma_loss: 0.2503 - val_loss: 0.2977 - val_brownian_loss: 0.3016 - val_sigma_loss: 0.2627\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3122 - brownian_loss: 0.3172 - sigma_loss: 0.2672 - val_loss: 0.3526 - val_brownian_loss: 0.3565 - val_sigma_loss: 0.3178\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2775 - brownian_loss: 0.2824 - sigma_loss: 0.2331 - val_loss: 0.2768 - val_brownian_loss: 0.2797 - val_sigma_loss: 0.2506\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3098 - brownian_loss: 0.3136 - sigma_loss: 0.2759 - val_loss: 0.3329 - val_brownian_loss: 0.3405 - val_sigma_loss: 0.2643\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2925 - brownian_loss: 0.2966 - sigma_loss: 0.2561 - val_loss: 0.2870 - val_brownian_loss: 0.2935 - val_sigma_loss: 0.2287\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2891 - brownian_loss: 0.2931 - sigma_loss: 0.2528 - val_loss: 0.2757 - val_brownian_loss: 0.2795 - val_sigma_loss: 0.2412\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3218 - brownian_loss: 0.3263 - sigma_loss: 0.2817 - val_loss: 0.3301 - val_brownian_loss: 0.3366 - val_sigma_loss: 0.2715\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.2884 - brownian_loss: 0.2928 - sigma_loss: 0.2485 - val_loss: 0.2580 - val_brownian_loss: 0.2626 - val_sigma_loss: 0.2161\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3160 - brownian_loss: 0.3208 - sigma_loss: 0.2730 - val_loss: 0.3095 - val_brownian_loss: 0.3137 - val_sigma_loss: 0.2713\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2989 - brownian_loss: 0.3035 - sigma_loss: 0.2581 - val_loss: 0.3034 - val_brownian_loss: 0.3119 - val_sigma_loss: 0.2267\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2861 - brownian_loss: 0.2899 - sigma_loss: 0.2521 - val_loss: 0.2673 - val_brownian_loss: 0.2701 - val_sigma_loss: 0.2421\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3114 - brownian_loss: 0.3153 - sigma_loss: 0.2769 - val_loss: 0.3070 - val_brownian_loss: 0.3121 - val_sigma_loss: 0.2610\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2841 - brownian_loss: 0.2886 - sigma_loss: 0.2439 - val_loss: 0.2790 - val_brownian_loss: 0.2837 - val_sigma_loss: 0.2367\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3173 - brownian_loss: 0.3215 - sigma_loss: 0.2791 - val_loss: 0.3297 - val_brownian_loss: 0.3339 - val_sigma_loss: 0.2926\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2982 - brownian_loss: 0.3026 - sigma_loss: 0.2583 - val_loss: 0.3114 - val_brownian_loss: 0.3176 - val_sigma_loss: 0.2553\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2906 - brownian_loss: 0.2944 - sigma_loss: 0.2565 - val_loss: 0.2869 - val_brownian_loss: 0.2915 - val_sigma_loss: 0.2460\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3185 - brownian_loss: 0.3232 - sigma_loss: 0.2764 - val_loss: 0.3061 - val_brownian_loss: 0.3095 - val_sigma_loss: 0.2755\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2857 - brownian_loss: 0.2898 - sigma_loss: 0.2491 - val_loss: 0.3132 - val_brownian_loss: 0.3141 - val_sigma_loss: 0.3051\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3146 - brownian_loss: 0.3187 - sigma_loss: 0.2776 - val_loss: 0.2793 - val_brownian_loss: 0.2834 - val_sigma_loss: 0.2427\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 70s - loss: 0.2939 - brownian_loss: 0.2988 - sigma_loss: 0.2497 - val_loss: 0.2566 - val_brownian_loss: 0.2606 - val_sigma_loss: 0.2205\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2947 - brownian_loss: 0.2990 - sigma_loss: 0.2565 - val_loss: 0.3053 - val_brownian_loss: 0.3101 - val_sigma_loss: 0.2618\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3169 - brownian_loss: 0.3213 - sigma_loss: 0.2774 - val_loss: 0.3104 - val_brownian_loss: 0.3150 - val_sigma_loss: 0.2692\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2797 - brownian_loss: 0.2841 - sigma_loss: 0.2393 - val_loss: 0.2595 - val_brownian_loss: 0.2603 - val_sigma_loss: 0.2524\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3156 - brownian_loss: 0.3194 - sigma_loss: 0.2809 - val_loss: 0.3373 - val_brownian_loss: 0.3406 - val_sigma_loss: 0.3074\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2902 - brownian_loss: 0.2952 - sigma_loss: 0.2445 - val_loss: 0.3280 - val_brownian_loss: 0.3310 - val_sigma_loss: 0.3009\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2850 - brownian_loss: 0.2893 - sigma_loss: 0.2464 - val_loss: 0.2891 - val_brownian_loss: 0.2942 - val_sigma_loss: 0.2431\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3177 - brownian_loss: 0.3221 - sigma_loss: 0.2781 - val_loss: 0.3048 - val_brownian_loss: 0.3068 - val_sigma_loss: 0.2864\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3042 - brownian_loss: 0.3085 - sigma_loss: 0.2655 - val_loss: 0.3249 - val_brownian_loss: 0.3307 - val_sigma_loss: 0.2731\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3013 - brownian_loss: 0.3060 - sigma_loss: 0.2589 - val_loss: 0.3073 - val_brownian_loss: 0.3139 - val_sigma_loss: 0.2484\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2883 - brownian_loss: 0.2923 - sigma_loss: 0.2527 - val_loss: 0.3178 - val_brownian_loss: 0.3228 - val_sigma_loss: 0.2729\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3210 - brownian_loss: 0.3259 - sigma_loss: 0.2765 - val_loss: 0.2846 - val_brownian_loss: 0.2881 - val_sigma_loss: 0.2533\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2785 - brownian_loss: 0.2829 - sigma_loss: 0.2397 - val_loss: 0.2816 - val_brownian_loss: 0.2835 - val_sigma_loss: 0.2650\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3101 - brownian_loss: 0.3145 - sigma_loss: 0.2702 - val_loss: 0.3133 - val_brownian_loss: 0.3150 - val_sigma_loss: 0.2976\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2918 - brownian_loss: 0.2960 - sigma_loss: 0.2542 - val_loss: 0.3062 - val_brownian_loss: 0.3136 - val_sigma_loss: 0.2400\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2929 - brownian_loss: 0.2966 - sigma_loss: 0.2590 - val_loss: 0.2929 - val_brownian_loss: 0.2962 - val_sigma_loss: 0.2638\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 23s - loss: 0.3130 - brownian_loss: 0.3176 - sigma_loss: 0.2721 - val_loss: 0.3046 - val_brownian_loss: 0.3056 - val_sigma_loss: 0.2959\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2822 - brownian_loss: 0.2868 - sigma_loss: 0.2409 - val_loss: 0.2982 - val_brownian_loss: 0.3028 - val_sigma_loss: 0.2569\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3100 - brownian_loss: 0.3141 - sigma_loss: 0.2738 - val_loss: 0.3590 - val_brownian_loss: 0.3638 - val_sigma_loss: 0.3157\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2969 - brownian_loss: 0.3012 - sigma_loss: 0.2585 - val_loss: 0.3112 - val_brownian_loss: 0.3141 - val_sigma_loss: 0.2845\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2833 - brownian_loss: 0.2872 - sigma_loss: 0.2473 - val_loss: 0.2789 - val_brownian_loss: 0.2812 - val_sigma_loss: 0.2583\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3200 - brownian_loss: 0.3236 - sigma_loss: 0.2871 - val_loss: 0.3224 - val_brownian_loss: 0.3247 - val_sigma_loss: 0.3020\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2875 - brownian_loss: 0.2916 - sigma_loss: 0.2506 - val_loss: 0.3117 - val_brownian_loss: 0.3156 - val_sigma_loss: 0.2768\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3080 - brownian_loss: 0.3122 - sigma_loss: 0.2703 - val_loss: 0.2643 - val_brownian_loss: 0.2680 - val_sigma_loss: 0.2310\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2909 - brownian_loss: 0.2961 - sigma_loss: 0.2442 - val_loss: 0.2590 - val_brownian_loss: 0.2616 - val_sigma_loss: 0.2358\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2856 - brownian_loss: 0.2897 - sigma_loss: 0.2479 - val_loss: 0.2662 - val_brownian_loss: 0.2692 - val_sigma_loss: 0.2393\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3198 - brownian_loss: 0.3239 - sigma_loss: 0.2834 - val_loss: 0.3123 - val_brownian_loss: 0.3144 - val_sigma_loss: 0.2937\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2764 - brownian_loss: 0.2809 - sigma_loss: 0.2359 - val_loss: 0.3185 - val_brownian_loss: 0.3214 - val_sigma_loss: 0.2922\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3085 - brownian_loss: 0.3129 - sigma_loss: 0.2690 - val_loss: 0.3057 - val_brownian_loss: 0.3111 - val_sigma_loss: 0.2569\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2929 - brownian_loss: 0.2974 - sigma_loss: 0.2518 - val_loss: 0.3278 - val_brownian_loss: 0.3348 - val_sigma_loss: 0.2647\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.3024 - brownian_loss: 0.3064 - sigma_loss: 0.2662 - val_loss: 0.2915 - val_brownian_loss: 0.2939 - val_sigma_loss: 0.2698\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3180 - brownian_loss: 0.3220 - sigma_loss: 0.2816 - val_loss: 0.3003 - val_brownian_loss: 0.3059 - val_sigma_loss: 0.2500\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2817 - brownian_loss: 0.2859 - sigma_loss: 0.2438 - val_loss: 0.3220 - val_brownian_loss: 0.3273 - val_sigma_loss: 0.2738\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3189 - brownian_loss: 0.3228 - sigma_loss: 0.2833 - val_loss: 0.3396 - val_brownian_loss: 0.3431 - val_sigma_loss: 0.3088\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2974 - brownian_loss: 0.3019 - sigma_loss: 0.2563 - val_loss: 0.3014 - val_brownian_loss: 0.3058 - val_sigma_loss: 0.2618\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2942 - brownian_loss: 0.2985 - sigma_loss: 0.2559 - val_loss: 0.3152 - val_brownian_loss: 0.3176 - val_sigma_loss: 0.2932\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3124 - brownian_loss: 0.3169 - sigma_loss: 0.2721 - val_loss: 0.3282 - val_brownian_loss: 0.3331 - val_sigma_loss: 0.2838\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2810 - brownian_loss: 0.2855 - sigma_loss: 0.2400 - val_loss: 0.2816 - val_brownian_loss: 0.2853 - val_sigma_loss: 0.2484\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3168 - brownian_loss: 0.3214 - sigma_loss: 0.2755 - val_loss: 0.2788 - val_brownian_loss: 0.2821 - val_sigma_loss: 0.2485\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 70s - loss: 0.3086 - brownian_loss: 0.3137 - sigma_loss: 0.2627 - val_loss: 0.3208 - val_brownian_loss: 0.3267 - val_sigma_loss: 0.2681\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2899 - brownian_loss: 0.2938 - sigma_loss: 0.2546 - val_loss: 0.2798 - val_brownian_loss: 0.2871 - val_sigma_loss: 0.2135\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3164 - brownian_loss: 0.3199 - sigma_loss: 0.2845 - val_loss: 0.3531 - val_brownian_loss: 0.3593 - val_sigma_loss: 0.2975\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2831 - brownian_loss: 0.2878 - sigma_loss: 0.2404 - val_loss: 0.2830 - val_brownian_loss: 0.2856 - val_sigma_loss: 0.2595\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3067 - brownian_loss: 0.3108 - sigma_loss: 0.2694 - val_loss: 0.3003 - val_brownian_loss: 0.3040 - val_sigma_loss: 0.2671\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2910 - brownian_loss: 0.2955 - sigma_loss: 0.2500 - val_loss: 0.3000 - val_brownian_loss: 0.3041 - val_sigma_loss: 0.2636\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2841 - brownian_loss: 0.2886 - sigma_loss: 0.2438 - val_loss: 0.3249 - val_brownian_loss: 0.3274 - val_sigma_loss: 0.3028\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3248 - brownian_loss: 0.3286 - sigma_loss: 0.2902 - val_loss: 0.3257 - val_brownian_loss: 0.3276 - val_sigma_loss: 0.3091\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2756 - brownian_loss: 0.2802 - sigma_loss: 0.2338 - val_loss: 0.2712 - val_brownian_loss: 0.2731 - val_sigma_loss: 0.2542\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3108 - brownian_loss: 0.3154 - sigma_loss: 0.2695 - val_loss: 0.2789 - val_brownian_loss: 0.2817 - val_sigma_loss: 0.2536\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2996 - brownian_loss: 0.3041 - sigma_loss: 0.2589 - val_loss: 0.2672 - val_brownian_loss: 0.2719 - val_sigma_loss: 0.2249\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2908 - brownian_loss: 0.2948 - sigma_loss: 0.2540 - val_loss: 0.3141 - val_brownian_loss: 0.3203 - val_sigma_loss: 0.2580\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3203 - brownian_loss: 0.3242 - sigma_loss: 0.2857 - val_loss: 0.3220 - val_brownian_loss: 0.3264 - val_sigma_loss: 0.2826\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 139s - loss: 0.2800 - brownian_loss: 0.2841 - sigma_loss: 0.2428 - val_loss: 0.2837 - val_brownian_loss: 0.2878 - val_sigma_loss: 0.2469\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3185 - brownian_loss: 0.3220 - sigma_loss: 0.2866 - val_loss: 0.3374 - val_brownian_loss: 0.3418 - val_sigma_loss: 0.2976\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2913 - brownian_loss: 0.2960 - sigma_loss: 0.2486 - val_loss: 0.3018 - val_brownian_loss: 0.3090 - val_sigma_loss: 0.2372\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2969 - brownian_loss: 0.3015 - sigma_loss: 0.2562 - val_loss: 0.3060 - val_brownian_loss: 0.3128 - val_sigma_loss: 0.2447\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3132 - brownian_loss: 0.3179 - sigma_loss: 0.2702 - val_loss: 0.3171 - val_brownian_loss: 0.3191 - val_sigma_loss: 0.2986\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2942 - brownian_loss: 0.2986 - sigma_loss: 0.2549 - val_loss: 0.2904 - val_brownian_loss: 0.2917 - val_sigma_loss: 0.2780\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3219 - brownian_loss: 0.3264 - sigma_loss: 0.2813 - val_loss: 0.3201 - val_brownian_loss: 0.3234 - val_sigma_loss: 0.2905\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2924 - brownian_loss: 0.2974 - sigma_loss: 0.2472 - val_loss: 0.3050 - val_brownian_loss: 0.3072 - val_sigma_loss: 0.2855\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2792 - brownian_loss: 0.2828 - sigma_loss: 0.2471 - val_loss: 0.2892 - val_brownian_loss: 0.2933 - val_sigma_loss: 0.2523\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3099 - brownian_loss: 0.3144 - sigma_loss: 0.2696 - val_loss: 0.2931 - val_brownian_loss: 0.2981 - val_sigma_loss: 0.2481\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2810 - brownian_loss: 0.2850 - sigma_loss: 0.2454 - val_loss: 0.2686 - val_brownian_loss: 0.2726 - val_sigma_loss: 0.2329\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3176 - brownian_loss: 0.3218 - sigma_loss: 0.2793 - val_loss: 0.3404 - val_brownian_loss: 0.3445 - val_sigma_loss: 0.3028\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2943 - brownian_loss: 0.2991 - sigma_loss: 0.2507 - val_loss: 0.2838 - val_brownian_loss: 0.2899 - val_sigma_loss: 0.2287\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2836 - brownian_loss: 0.2877 - sigma_loss: 0.2460 - val_loss: 0.2882 - val_brownian_loss: 0.2928 - val_sigma_loss: 0.2470\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3144 - brownian_loss: 0.3190 - sigma_loss: 0.2726 - val_loss: 0.3013 - val_brownian_loss: 0.3052 - val_sigma_loss: 0.2662\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2863 - brownian_loss: 0.2905 - sigma_loss: 0.2491 - val_loss: 0.3010 - val_brownian_loss: 0.3044 - val_sigma_loss: 0.2710\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3108 - brownian_loss: 0.3155 - sigma_loss: 0.2684 - val_loss: 0.3099 - val_brownian_loss: 0.3173 - val_sigma_loss: 0.2431\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2941 - brownian_loss: 0.2986 - sigma_loss: 0.2540 - val_loss: 0.2961 - val_brownian_loss: 0.2995 - val_sigma_loss: 0.2652\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2865 - brownian_loss: 0.2902 - sigma_loss: 0.2536 - val_loss: 0.2454 - val_brownian_loss: 0.2484 - val_sigma_loss: 0.2183\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3231 - brownian_loss: 0.3277 - sigma_loss: 0.2823 - val_loss: 0.3377 - val_brownian_loss: 0.3412 - val_sigma_loss: 0.3067\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2703 - brownian_loss: 0.2743 - sigma_loss: 0.2346 - val_loss: 0.2955 - val_brownian_loss: 0.2993 - val_sigma_loss: 0.2609\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3141 - brownian_loss: 0.3178 - sigma_loss: 0.2803 - val_loss: 0.3301 - val_brownian_loss: 0.3351 - val_sigma_loss: 0.2850\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2953 - brownian_loss: 0.2998 - sigma_loss: 0.2550 - val_loss: 0.2720 - val_brownian_loss: 0.2743 - val_sigma_loss: 0.2508\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2845 - brownian_loss: 0.2882 - sigma_loss: 0.2514 - val_loss: 0.3038 - val_brownian_loss: 0.3070 - val_sigma_loss: 0.2744\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3241 - brownian_loss: 0.3288 - sigma_loss: 0.2826 - val_loss: 0.3118 - val_brownian_loss: 0.3181 - val_sigma_loss: 0.2548\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2849 - brownian_loss: 0.2895 - sigma_loss: 0.2439 - val_loss: 0.2374 - val_brownian_loss: 0.2421 - val_sigma_loss: 0.1952\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3160 - brownian_loss: 0.3200 - sigma_loss: 0.2797 - val_loss: 0.3190 - val_brownian_loss: 0.3220 - val_sigma_loss: 0.2911\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2967 - brownian_loss: 0.3024 - sigma_loss: 0.2457 - val_loss: 0.3067 - val_brownian_loss: 0.3117 - val_sigma_loss: 0.2618\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2850 - brownian_loss: 0.2886 - sigma_loss: 0.2519 - val_loss: 0.2910 - val_brownian_loss: 0.2950 - val_sigma_loss: 0.2551\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3155 - brownian_loss: 0.3201 - sigma_loss: 0.2745 - val_loss: 0.3272 - val_brownian_loss: 0.3303 - val_sigma_loss: 0.2995\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 139s - loss: 0.2850 - brownian_loss: 0.2893 - sigma_loss: 0.2462 - val_loss: 0.2621 - val_brownian_loss: 0.2664 - val_sigma_loss: 0.2238\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3109 - brownian_loss: 0.3157 - sigma_loss: 0.2676 - val_loss: 0.3139 - val_brownian_loss: 0.3185 - val_sigma_loss: 0.2728\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2963 - brownian_loss: 0.3007 - sigma_loss: 0.2560 - val_loss: 0.3093 - val_brownian_loss: 0.3143 - val_sigma_loss: 0.2644\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2869 - brownian_loss: 0.2915 - sigma_loss: 0.2450 - val_loss: 0.2933 - val_brownian_loss: 0.2964 - val_sigma_loss: 0.2654\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3170 - brownian_loss: 0.3206 - sigma_loss: 0.2848 - val_loss: 0.3062 - val_brownian_loss: 0.3100 - val_sigma_loss: 0.2725\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2829 - brownian_loss: 0.2872 - sigma_loss: 0.2442 - val_loss: 0.2771 - val_brownian_loss: 0.2814 - val_sigma_loss: 0.2379\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3106 - brownian_loss: 0.3152 - sigma_loss: 0.2692 - val_loss: 0.2941 - val_brownian_loss: 0.2984 - val_sigma_loss: 0.2558\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2959 - brownian_loss: 0.3002 - sigma_loss: 0.2569 - val_loss: 0.3088 - val_brownian_loss: 0.3130 - val_sigma_loss: 0.2713\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2876 - brownian_loss: 0.2921 - sigma_loss: 0.2469 - val_loss: 0.2647 - val_brownian_loss: 0.2700 - val_sigma_loss: 0.2164\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3171 - brownian_loss: 0.3208 - sigma_loss: 0.2830 - val_loss: 0.3242 - val_brownian_loss: 0.3258 - val_sigma_loss: 0.3097\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2783 - brownian_loss: 0.2830 - sigma_loss: 0.2368 - val_loss: 0.2879 - val_brownian_loss: 0.2919 - val_sigma_loss: 0.2523\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3159 - brownian_loss: 0.3204 - sigma_loss: 0.2753 - val_loss: 0.3228 - val_brownian_loss: 0.3285 - val_sigma_loss: 0.2713\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2931 - brownian_loss: 0.2975 - sigma_loss: 0.2544 - val_loss: 0.2895 - val_brownian_loss: 0.2921 - val_sigma_loss: 0.2662\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2916 - brownian_loss: 0.2956 - sigma_loss: 0.2551 - val_loss: 0.3046 - val_brownian_loss: 0.3083 - val_sigma_loss: 0.2713\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3224 - brownian_loss: 0.3265 - sigma_loss: 0.2850 - val_loss: 0.2672 - val_brownian_loss: 0.2702 - val_sigma_loss: 0.2397\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2803 - brownian_loss: 0.2845 - sigma_loss: 0.2424 - val_loss: 0.2736 - val_brownian_loss: 0.2765 - val_sigma_loss: 0.2469\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3155 - brownian_loss: 0.3202 - sigma_loss: 0.2732 - val_loss: 0.3060 - val_brownian_loss: 0.3087 - val_sigma_loss: 0.2817\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3043 - brownian_loss: 0.3092 - sigma_loss: 0.2601 - val_loss: 0.2972 - val_brownian_loss: 0.3052 - val_sigma_loss: 0.2248\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2860 - brownian_loss: 0.2901 - sigma_loss: 0.2488 - val_loss: 0.2962 - val_brownian_loss: 0.3006 - val_sigma_loss: 0.2566\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3250 - brownian_loss: 0.3291 - sigma_loss: 0.2876 - val_loss: 0.3120 - val_brownian_loss: 0.3189 - val_sigma_loss: 0.2498\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 144s - loss: 0.2777 - brownian_loss: 0.2817 - sigma_loss: 0.2417 - val_loss: 0.2719 - val_brownian_loss: 0.2754 - val_sigma_loss: 0.2408\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3251 - brownian_loss: 0.3293 - sigma_loss: 0.2873 - val_loss: 0.3203 - val_brownian_loss: 0.3245 - val_sigma_loss: 0.2827\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2954 - brownian_loss: 0.3001 - sigma_loss: 0.2529 - val_loss: 0.2592 - val_brownian_loss: 0.2639 - val_sigma_loss: 0.2177\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2886 - brownian_loss: 0.2927 - sigma_loss: 0.2520 - val_loss: 0.3035 - val_brownian_loss: 0.3094 - val_sigma_loss: 0.2506\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3179 - brownian_loss: 0.3227 - sigma_loss: 0.2744 - val_loss: 0.3113 - val_brownian_loss: 0.3134 - val_sigma_loss: 0.2919\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2800 - brownian_loss: 0.2840 - sigma_loss: 0.2438 - val_loss: 0.2875 - val_brownian_loss: 0.2906 - val_sigma_loss: 0.2598\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3085 - brownian_loss: 0.3129 - sigma_loss: 0.2695 - val_loss: 0.2911 - val_brownian_loss: 0.2972 - val_sigma_loss: 0.2358\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.3020 - brownian_loss: 0.3071 - sigma_loss: 0.2562 - val_loss: 0.2842 - val_brownian_loss: 0.2892 - val_sigma_loss: 0.2394\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 114s - loss: 0.2943 - brownian_loss: 0.2984 - sigma_loss: 0.2577 - val_loss: 0.2681 - val_brownian_loss: 0.2727 - val_sigma_loss: 0.2260\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3228 - brownian_loss: 0.3269 - sigma_loss: 0.2861 - val_loss: 0.3364 - val_brownian_loss: 0.3438 - val_sigma_loss: 0.2700\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2812 - brownian_loss: 0.2851 - sigma_loss: 0.2454 - val_loss: 0.2882 - val_brownian_loss: 0.2925 - val_sigma_loss: 0.2490\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3202 - brownian_loss: 0.3241 - sigma_loss: 0.2850 - val_loss: 0.3301 - val_brownian_loss: 0.3332 - val_sigma_loss: 0.3023\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.3066 - brownian_loss: 0.3116 - sigma_loss: 0.2616 - val_loss: 0.3099 - val_brownian_loss: 0.3139 - val_sigma_loss: 0.2735\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2883 - brownian_loss: 0.2921 - sigma_loss: 0.2546 - val_loss: 0.2848 - val_brownian_loss: 0.2866 - val_sigma_loss: 0.2685\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3218 - brownian_loss: 0.3259 - sigma_loss: 0.2854 - val_loss: 0.3680 - val_brownian_loss: 0.3737 - val_sigma_loss: 0.3171\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 146s - loss: 0.2813 - brownian_loss: 0.2857 - sigma_loss: 0.2418 - val_loss: 0.2592 - val_brownian_loss: 0.2625 - val_sigma_loss: 0.2294\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3121 - brownian_loss: 0.3162 - sigma_loss: 0.2760 - val_loss: 0.3474 - val_brownian_loss: 0.3518 - val_sigma_loss: 0.3073\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2934 - brownian_loss: 0.2981 - sigma_loss: 0.2511 - val_loss: 0.2809 - val_brownian_loss: 0.2832 - val_sigma_loss: 0.2596\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2886 - brownian_loss: 0.2926 - sigma_loss: 0.2520 - val_loss: 0.3193 - val_brownian_loss: 0.3249 - val_sigma_loss: 0.2684\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3089 - brownian_loss: 0.3132 - sigma_loss: 0.2695 - val_loss: 0.3367 - val_brownian_loss: 0.3389 - val_sigma_loss: 0.3164\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2823 - brownian_loss: 0.2867 - sigma_loss: 0.2431 - val_loss: 0.2467 - val_brownian_loss: 0.2495 - val_sigma_loss: 0.2215\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3086 - brownian_loss: 0.3130 - sigma_loss: 0.2687 - val_loss: 0.3023 - val_brownian_loss: 0.3088 - val_sigma_loss: 0.2440\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2993 - brownian_loss: 0.3044 - sigma_loss: 0.2538 - val_loss: 0.3041 - val_brownian_loss: 0.3111 - val_sigma_loss: 0.2412\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 110s - loss: 0.2797 - brownian_loss: 0.2841 - sigma_loss: 0.2400 - val_loss: 0.3029 - val_brownian_loss: 0.3095 - val_sigma_loss: 0.2429\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3230 - brownian_loss: 0.3273 - sigma_loss: 0.2844 - val_loss: 0.2832 - val_brownian_loss: 0.2834 - val_sigma_loss: 0.2816\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2790 - brownian_loss: 0.2838 - sigma_loss: 0.2362 - val_loss: 0.2914 - val_brownian_loss: 0.2953 - val_sigma_loss: 0.2561\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3142 - brownian_loss: 0.3191 - sigma_loss: 0.2702 - val_loss: 0.3351 - val_brownian_loss: 0.3402 - val_sigma_loss: 0.2894\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2974 - brownian_loss: 0.3024 - sigma_loss: 0.2518 - val_loss: 0.3067 - val_brownian_loss: 0.3115 - val_sigma_loss: 0.2641\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2871 - brownian_loss: 0.2916 - sigma_loss: 0.2464 - val_loss: 0.2895 - val_brownian_loss: 0.2923 - val_sigma_loss: 0.2647\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3170 - brownian_loss: 0.3210 - sigma_loss: 0.2813 - val_loss: 0.3280 - val_brownian_loss: 0.3321 - val_sigma_loss: 0.2911\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 139s - loss: 0.2814 - brownian_loss: 0.2858 - sigma_loss: 0.2422 - val_loss: 0.3026 - val_brownian_loss: 0.3048 - val_sigma_loss: 0.2826\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3073 - brownian_loss: 0.3118 - sigma_loss: 0.2671 - val_loss: 0.3246 - val_brownian_loss: 0.3268 - val_sigma_loss: 0.3045\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2871 - brownian_loss: 0.2921 - sigma_loss: 0.2419 - val_loss: 0.2863 - val_brownian_loss: 0.2924 - val_sigma_loss: 0.2315\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2920 - brownian_loss: 0.2960 - sigma_loss: 0.2555 - val_loss: 0.2731 - val_brownian_loss: 0.2754 - val_sigma_loss: 0.2530\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3184 - brownian_loss: 0.3224 - sigma_loss: 0.2821 - val_loss: 0.3499 - val_brownian_loss: 0.3503 - val_sigma_loss: 0.3456\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2801 - brownian_loss: 0.2849 - sigma_loss: 0.2372 - val_loss: 0.2870 - val_brownian_loss: 0.2909 - val_sigma_loss: 0.2521\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3051 - brownian_loss: 0.3099 - sigma_loss: 0.2617 - val_loss: 0.3088 - val_brownian_loss: 0.3124 - val_sigma_loss: 0.2762\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2912 - brownian_loss: 0.2956 - sigma_loss: 0.2512 - val_loss: 0.3190 - val_brownian_loss: 0.3246 - val_sigma_loss: 0.2687\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2925 - brownian_loss: 0.2961 - sigma_loss: 0.2604 - val_loss: 0.2784 - val_brownian_loss: 0.2815 - val_sigma_loss: 0.2504\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3251 - brownian_loss: 0.3285 - sigma_loss: 0.2950 - val_loss: 0.3446 - val_brownian_loss: 0.3479 - val_sigma_loss: 0.3150\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2800 - brownian_loss: 0.2848 - sigma_loss: 0.2362 - val_loss: 0.3015 - val_brownian_loss: 0.3030 - val_sigma_loss: 0.2877\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 37s - loss: 0.3168 - brownian_loss: 0.3211 - sigma_loss: 0.2784 - val_loss: 0.3157 - val_brownian_loss: 0.3198 - val_sigma_loss: 0.2787\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2950 - brownian_loss: 0.2999 - sigma_loss: 0.2507 - val_loss: 0.2816 - val_brownian_loss: 0.2869 - val_sigma_loss: 0.2346\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 109s - loss: 0.2835 - brownian_loss: 0.2880 - sigma_loss: 0.2435 - val_loss: 0.2773 - val_brownian_loss: 0.2782 - val_sigma_loss: 0.2687\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3194 - brownian_loss: 0.3238 - sigma_loss: 0.2798 - val_loss: 0.3371 - val_brownian_loss: 0.3435 - val_sigma_loss: 0.2795\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 140s - loss: 0.2807 - brownian_loss: 0.2848 - sigma_loss: 0.2437 - val_loss: 0.2638 - val_brownian_loss: 0.2665 - val_sigma_loss: 0.2390\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3121 - brownian_loss: 0.3168 - sigma_loss: 0.2695 - val_loss: 0.3277 - val_brownian_loss: 0.3325 - val_sigma_loss: 0.2842\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2919 - brownian_loss: 0.2963 - sigma_loss: 0.2516 - val_loss: 0.2805 - val_brownian_loss: 0.2858 - val_sigma_loss: 0.2322\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2883 - brownian_loss: 0.2928 - sigma_loss: 0.2476 - val_loss: 0.2715 - val_brownian_loss: 0.2767 - val_sigma_loss: 0.2244\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3175 - brownian_loss: 0.3221 - sigma_loss: 0.2754 - val_loss: 0.3268 - val_brownian_loss: 0.3295 - val_sigma_loss: 0.3026\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3161 - brownian_loss: 0.3204 - sigma_loss: 0.2778 - val_loss: 0.3113 - val_brownian_loss: 0.3168 - val_sigma_loss: 0.2622\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2974 - brownian_loss: 0.3025 - sigma_loss: 0.2521 - val_loss: 0.3075 - val_brownian_loss: 0.3130 - val_sigma_loss: 0.2576\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 112s - loss: 0.2884 - brownian_loss: 0.2927 - sigma_loss: 0.2502 - val_loss: 0.3086 - val_brownian_loss: 0.3151 - val_sigma_loss: 0.2503\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3126 - brownian_loss: 0.3171 - sigma_loss: 0.2719 - val_loss: 0.3137 - val_brownian_loss: 0.3173 - val_sigma_loss: 0.2811\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 147s - loss: 0.2803 - brownian_loss: 0.2848 - sigma_loss: 0.2396 - val_loss: 0.2495 - val_brownian_loss: 0.2533 - val_sigma_loss: 0.2157\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 40s - loss: 0.3180 - brownian_loss: 0.3229 - sigma_loss: 0.2740 - val_loss: 0.3213 - val_brownian_loss: 0.3270 - val_sigma_loss: 0.2693\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 75s - loss: 0.2978 - brownian_loss: 0.3025 - sigma_loss: 0.2554 - val_loss: 0.3081 - val_brownian_loss: 0.3131 - val_sigma_loss: 0.2629\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2776 - brownian_loss: 0.2822 - sigma_loss: 0.2362 - val_loss: 0.2656 - val_brownian_loss: 0.2700 - val_sigma_loss: 0.2264\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3144 - brownian_loss: 0.3190 - sigma_loss: 0.2725 - val_loss: 0.3092 - val_brownian_loss: 0.3153 - val_sigma_loss: 0.2544\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2831 - brownian_loss: 0.2877 - sigma_loss: 0.2424 - val_loss: 0.2493 - val_brownian_loss: 0.2544 - val_sigma_loss: 0.2039\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3026 - brownian_loss: 0.3065 - sigma_loss: 0.2673 - val_loss: 0.2761 - val_brownian_loss: 0.2798 - val_sigma_loss: 0.2420\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2976 - brownian_loss: 0.3025 - sigma_loss: 0.2535 - val_loss: 0.2833 - val_brownian_loss: 0.2874 - val_sigma_loss: 0.2456\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2843 - brownian_loss: 0.2878 - sigma_loss: 0.2528 - val_loss: 0.2814 - val_brownian_loss: 0.2866 - val_sigma_loss: 0.2344\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3219 - brownian_loss: 0.3263 - sigma_loss: 0.2820 - val_loss: 0.3369 - val_brownian_loss: 0.3412 - val_sigma_loss: 0.2976\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 141s - loss: 0.2797 - brownian_loss: 0.2839 - sigma_loss: 0.2423 - val_loss: 0.2667 - val_brownian_loss: 0.2706 - val_sigma_loss: 0.2321\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3110 - brownian_loss: 0.3159 - sigma_loss: 0.2663 - val_loss: 0.3139 - val_brownian_loss: 0.3167 - val_sigma_loss: 0.2889\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 74s - loss: 0.2994 - brownian_loss: 0.3043 - sigma_loss: 0.2557 - val_loss: 0.3404 - val_brownian_loss: 0.3467 - val_sigma_loss: 0.2844\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2869 - brownian_loss: 0.2909 - sigma_loss: 0.2511 - val_loss: 0.2711 - val_brownian_loss: 0.2742 - val_sigma_loss: 0.2431\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3250 - brownian_loss: 0.3291 - sigma_loss: 0.2874 - val_loss: 0.3232 - val_brownian_loss: 0.3274 - val_sigma_loss: 0.2857\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 145s - loss: 0.2862 - brownian_loss: 0.2901 - sigma_loss: 0.2512 - val_loss: 0.2944 - val_brownian_loss: 0.2996 - val_sigma_loss: 0.2469\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3096 - brownian_loss: 0.3150 - sigma_loss: 0.2611 - val_loss: 0.3003 - val_brownian_loss: 0.3061 - val_sigma_loss: 0.2481\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 72s - loss: 0.2929 - brownian_loss: 0.2972 - sigma_loss: 0.2551 - val_loss: 0.2694 - val_brownian_loss: 0.2731 - val_sigma_loss: 0.2366\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2868 - brownian_loss: 0.2912 - sigma_loss: 0.2470 - val_loss: 0.2899 - val_brownian_loss: 0.2927 - val_sigma_loss: 0.2648\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3254 - brownian_loss: 0.3300 - sigma_loss: 0.2834 - val_loss: 0.3249 - val_brownian_loss: 0.3302 - val_sigma_loss: 0.2775\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2828 - brownian_loss: 0.2871 - sigma_loss: 0.2440 - val_loss: 0.2635 - val_brownian_loss: 0.2682 - val_sigma_loss: 0.2213\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 39s - loss: 0.3138 - brownian_loss: 0.3185 - sigma_loss: 0.2714 - val_loss: 0.3353 - val_brownian_loss: 0.3408 - val_sigma_loss: 0.2856\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 73s - loss: 0.2989 - brownian_loss: 0.3035 - sigma_loss: 0.2569 - val_loss: 0.3288 - val_brownian_loss: 0.3325 - val_sigma_loss: 0.2958\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 113s - loss: 0.2834 - brownian_loss: 0.2881 - sigma_loss: 0.2415 - val_loss: 0.2936 - val_brownian_loss: 0.2974 - val_sigma_loss: 0.2596\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 22s - loss: 0.3152 - brownian_loss: 0.3198 - sigma_loss: 0.2743 - val_loss: 0.2759 - val_brownian_loss: 0.2828 - val_sigma_loss: 0.2134\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 143s - loss: 0.2859 - brownian_loss: 0.2896 - sigma_loss: 0.2519 - val_loss: 0.2916 - val_brownian_loss: 0.2981 - val_sigma_loss: 0.2330\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 38s - loss: 0.3127 - brownian_loss: 0.3168 - sigma_loss: 0.2753 - val_loss: 0.3093 - val_brownian_loss: 0.3115 - val_sigma_loss: 0.2893\n",
      "100\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 71s - loss: 0.2948 - brownian_loss: 0.3001 - sigma_loss: 0.2478 - val_loss: 0.2518 - val_brownian_loss: 0.2541 - val_sigma_loss: 0.2316\n",
      "150\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 111s - loss: 0.2902 - brownian_loss: 0.2937 - sigma_loss: 0.2583 - val_loss: 0.2685 - val_brownian_loss: 0.2738 - val_sigma_loss: 0.2213\n",
      "26\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 21s - loss: 0.3181 - brownian_loss: 0.3223 - sigma_loss: 0.2808 - val_loss: 0.3324 - val_brownian_loss: 0.3401 - val_sigma_loss: 0.2637\n",
      "200\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "3800/3800 [==============================] - 142s - loss: 0.2873 - brownian_loss: 0.2923 - sigma_loss: 0.2416 - val_loss: 0.2866 - val_brownian_loss: 0.2895 - val_sigma_loss: 0.2598\n",
      "50\n",
      "0.10000000149\n",
      "Train on 3800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "2750/3800 [====================>.........] - ETA: 10s - loss: 0.3152 - brownian_loss: 0.3200 - sigma_loss: 0.2716"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f86c3ce6d045>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#print ret.keys()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m#print ret[\"category\"].shape, ret[\"output\"].shape,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mwgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[history])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jarbona/keras2ConvR/keras/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1106\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jarbona/keras2ConvR/keras/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jarbona/keras2ConvR/keras/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from Specialist_layer import  return_three_layer,return_three_bis\n",
    "history = LossHistory(\"losses16.pick\")\n",
    "#TRaining of graph 1\n",
    "#print lr\n",
    "#lr = 0.1\n",
    "lr = 1.0\n",
    "wgraph = model\n",
    "wgraph.optimizer.lr.set_value(lr)\n",
    "for i in range(15):\n",
    "    for j in range(0,6*6*4,1):\n",
    "    \n",
    "        modulo = 5\n",
    "        size = (1 + j % modulo)*50\n",
    "       \n",
    "        if j % modulo == 4:\n",
    "            size=200\n",
    "        if j % modulo == 5:\n",
    "            size=400    \n",
    "        if j % modulo == 3:\n",
    "            size=26\n",
    "        #size=26\n",
    "        print size\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            X_train,Y_trains_b,Y_trains_s,scs = generate_N_nstep(4000,size)\n",
    "\n",
    "            inp = {\"Input\":X_train}\n",
    "            ret={\"brownian\":Y_trains_b,\n",
    "                \"sigma\":Y_trains_s}\n",
    "            \"\"\"#\n",
    "            ret = {\"input1\":X_train,\n",
    "                   \"output\":Y_trains,\n",
    "                   \"outputtype\":convert_output(Y_trains),\n",
    "                  \"category\":Y_train_cat[::,::,:12]}\"\"\"\n",
    "            \n",
    "        except:\n",
    "            print \"pb\"\n",
    "            pass\n",
    "        #next(generator())\n",
    "        \n",
    "        #print ret[\"category\"].shape\n",
    "        if size == 400:\n",
    "            wgraph.optimizer.lr.set_value(lr)\n",
    "            print wgraph.optimizer.lr.get_value()\n",
    "            \n",
    "        if size == 50:\n",
    "            wgraph.optimizer.lr.set_value(lr)\n",
    "            print wgraph.optimizer.lr.get_value()\n",
    "            \n",
    "        if size != 600:\n",
    "            batch = 50\n",
    "        else:\n",
    "            batch = 20\n",
    "        #print ret.keys()\n",
    "        #print ret[\"category\"].shape, ret[\"output\"].shape, \n",
    "        wgraph.fit(inp,ret,batch, nb_epoch=1,validation_split=0.05)#, callbacks=[history])\n",
    "        \n",
    "        if i == 3:\n",
    "            lr = 0.1\n",
    "        if j % modulo == 0 :\n",
    "            name = \"ftest_sigma\"\n",
    "            wgraph.save_weights(name + \"_%i_%i\"%(i+2,j),overwrite=True)\n",
    "            #sub_with_noise (30p)\n",
    "        \n",
    "            \n",
    "            #if np.isnan(graph.evaluate(ret)):\n",
    "            #    graph = return_three_layer()\n",
    "            #    graph.load_weights(\"transition_l8_%i_diff_size_50\"%(i+2))\n",
    "    #if i % 3 == 0 and i != 0:\n",
    "    #    lr /= 2.\n",
    "    #    graph.optimizer.lr.set_value(lr)\n",
    "    #    print graph.optimizer.lr.get_value()\n",
    "\n",
    "#score = model.evaluate(X_test, Y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,Y_trains_b,Y_trains_s,scs = generate_N_nstep(200,200)\n",
    "\n",
    "inp = {\"Input\":X_train}\n",
    "ret={\"brownian\":Y_trains_b,\n",
    "    \"sigma\":Y_trains_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp  = model.predict(inp[\"Input\"][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd878b27390>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEOCAYAAADYAlMOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni5K7300AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd877a3f5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEOCAYAAADc94MzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaACAgICArzE/wfFzNUG1F4ddAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8748643d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Tools import plot_by_class,plot_label\n",
    "import copy\n",
    "w = 16\n",
    "res = copy.deepcopy(resp[0][w])[::,0]\n",
    "res[res >0.5] = 1\n",
    "res[res<0.5] = 0\n",
    "gt = Y_trains_b[w][::,0]\n",
    "print res.shape\n",
    "print gt.shape\n",
    "#print resp[1][w][::,0]\n",
    "fig = figure()\n",
    "ax = fig.add_subplot(121)\n",
    "plot_label(np.cumsum(inp[\"Input\"][w][::,2:4],axis=0),res,remove6=9)\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "plot_label(np.cumsum(inp[\"Input\"][w][::,2:4],axis=0),gt,remove6=9)\n",
    "\n",
    "figure()\n",
    "plot(resp[1][w][::,0])\n",
    "plot(ret[\"sigma\"][w][::,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print ret[\"brownian\"][w][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
